<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Foobot Technical Blog</title>
    <description>The guys behind Foobot. Talking here about our achievements, failures &amp; OSS contributions
</description>
    <link>http://localhost:4000/</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Tue, 21 Nov 2023 12:45:22 +0100</pubDate>
    <lastBuildDate>Tue, 21 Nov 2023 12:45:22 +0100</lastBuildDate>
    <generator>Jekyll v4.3.2</generator>
    
      <item>
        <title>Masking for Representation Learning</title>
        <description>
&lt;style type=&quot;text/css&quot;&gt;
.center {
    display:block;
    margin: 0 auto;
}
.double-left {
    width:49%;
    display:block;
    float:left;
    margin: 0 auto;
    margin-right: 2rem;
}
.double-left-sm {
    width:25%;
    display:block;
    float:left;
    margin: 0 auto;
    margin-right: 2rem;
}
.double-right-m {
    width:35%;
    display:block;
    float:right;
    margin: 0 auto;
    margin-right: 2rem;
}
.add-margin-right {
    margin-right: 20px;
}
.double {
    width: 49%;
    margin: 0 auto;
}
.double-unconst {
    width: auto;
    margin: 0 auto;
    margin-left: 1.5rem;
}
.image-foot {
    font-size:10pt;
    max-width: 30rem;
    margin: 0 auto;
}
.small-img {
  width: 45%;
}
.small-txt {
  font-size: 0.8rem;
}
ul {
  display: table;
}
pre {
  white-space: pre-wrap;
  overflow-x: auto;
  margin: 1rem 0pt 1rem 0pt;
}
code {
  font-size: 0.9rem;
  margin: 1rem 0pt 1rem 0pt;
}
h1, h2, h3, h4 {
  margin: 1rem 0pt 1rem 0pt;
}
&lt;/style&gt;

&lt;h2 id=&quot;masking-in-transformer-based-reinforcement-learning&quot;&gt;Masking in Transformer-based Reinforcement Learning&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/masking/masking.gif&quot; alt=&quot;sources&quot; class=&quot;center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Reinforcement Learning (RL) has an history in using masking. &lt;a href=&quot;#1&quot;&gt;[1]&lt;/a&gt; and &lt;a href=&quot;#2&quot;&gt;[2]&lt;/a&gt; show how masking is used mainly to prevent taking invalid actions in a controlled environment.&lt;/p&gt;

&lt;p&gt;Here we’re going to focus on a more recent, slightly different but still related usage of masks in Transformers-based RL, through a series of code-documented examples.&lt;/p&gt;

&lt;h3 id=&quot;use-cases-of-transformers-in-rl&quot;&gt;Use cases of Transformers in RL&lt;/h3&gt;

&lt;p&gt;Several papers and studies demonstrate the efficiency of Transformers-based architectures employed as a component of neural networks used in classic RL algorithms such as PPO.&lt;/p&gt;

&lt;p&gt;For instance, &lt;a href=&quot;#3&quot;&gt;[3]&lt;/a&gt; empirically demonstrates its superiority over LSTM in challenging environments. &lt;a href=&quot;#4&quot;&gt;[4]&lt;/a&gt; explains why Transformers improve training results on memory-based tasks. In short, Transformers are very useful to give the model a memory and help with environment where state is partially visible.&lt;/p&gt;

&lt;p&gt;Recently, a new paradigm was introduced that heavily relies on Transformers capabilities to store massive amount of information and generalization across multiple execution contexts and tasks: the concept is named &lt;strong&gt;Decision Transformers&lt;/strong&gt; &lt;a href=&quot;#5&quot;&gt;[5]&lt;/a&gt;. It bridges the gap between sequence modeling problems and reinforcement learning, and attemtps to replicate impressive performance demonstrated on natural language processing or vision tasks to control optimization. It has known theoretical limitations (see for instance &lt;a href=&quot;#8&quot;&gt;[8]&lt;/a&gt;) but it nonetheless paved the way to several advancements.&lt;/p&gt;

&lt;center&gt;
$$ 
\begin{equation} 
\begin{split}
p(O_t | a_t, s_t) &amp;amp; \propto p(a_t | s_t, O_t) &amp;amp; \text{ Bayes} \\ 
&amp;amp; \Rightarrow p(a_t | s_t, R_t) &amp;amp; \text{ MDP} \\ 
&amp;amp; \Rightarrow p(a_t | s_t, R_t, H_t) &amp;amp; \text{ POMDP}
\end{split} \\
\text{with } H_t = a_{t-1}, s_{t-1}, R_{t-1}, ...
\end{equation}
$$
&lt;/center&gt;

&lt;p&gt;The main idea, summarized in equation $(1)$, is that &lt;strong&gt;finding control optimality given an action and a state is proportional to the probability of sampling an action given a state and a goal&lt;/strong&gt; in a Markov Decision Process (MDP), or an history of states, actions and returns in a Partially Observable MDP. In short, the probability distribution over actions is now both state and return-conditioned, or goal-conditioned.&lt;/p&gt;

&lt;p&gt;Compared to a Large Language Model (LLM), words or images tokens are replaced by trajectories made of actions, observations and returns. As for any other Transformers-based applications, the use of the self-attention mechanism is the core concept of the architecture, and we’ll see next &lt;strong&gt;how masks can be used in elegant and useful ways to fullfil specific training needs&lt;/strong&gt;.&lt;/p&gt;

&lt;h3 id=&quot;causal-mask-in-the-self-attention-mechanism&quot;&gt;Causal mask in the self-attention mechanism&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/masking/scaled_dot_product_attn.svg&quot; alt=&quot;sources&quot; class=&quot;double-left-sm&quot; /&gt;&lt;/p&gt;

&lt;div style=&quot;margin-top: 2rem&quot;&gt;&lt;/div&gt;

&lt;p&gt;The now famous “&lt;i&gt;Attention is all you need&lt;/i&gt;” paper &lt;a href=&quot;#13&quot;&gt;[13]&lt;/a&gt; introduces several concepts, including a causal mask which intends to hide future tokens we aim to predict: the model is allowed to look only at past tokens and current time step token to predict next token(s), the latter remainining hidden so that future tokens aren’t visible.&lt;/p&gt;

&lt;p&gt;Imagine training a model to generate the last token in a sentence: during training you’ll present first tokens in the sentence only and hide the last, otherwise it would be too simple and wouldn’t generalize well.&lt;/p&gt;

&lt;p&gt;Notice that this mask is marked optional in the graph, and can be modified to satisfy other purposes! That’s what we’ll be using in the next section.&lt;/p&gt;

&lt;p&gt;But for now, let’s see with an example how a basic causal mask can be constructed. Masking then using softmax are consecutive operations, which is equivalent to &lt;strong&gt;a weigthed average over all input vectors using lower triangular matrix to mask out future tokens&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;First a lower triangular matrix (with zeroes on the upper triangular part and ones on the lower part) is used to mask the future. Then, since the next operation is in the execution graph is a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;softmax&lt;/code&gt;, replacing all zeros by $-\infty$ will take advantage of the function $\frac{e^{z_{i}}}{\sum_{j=1}^K e^{z_{j}}}$, knowing that $\lim_{x \to -\infty}e^x = 0$&lt;/p&gt;

&lt;p&gt;Here is a toy example that shows how masking is used in the scaled dot-product attention (see &lt;a href=&quot;#14&quot;&gt;[14]&lt;/a&gt; for a thorough explanation):&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch.nn&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;functional&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;F&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;tril&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;tril&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;ones&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;wei&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;wei&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;wei&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;masked_fill&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tril&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;-inf&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;wei&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;F&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;softmax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;wei&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dim&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;wei&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;gives&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;tensor([[1.0000, 0.0000, 0.0000, 0.0000],
        [0.5000, 0.5000, 0.0000, 0.0000],
        [0.3333, 0.3333, 0.3333, 0.0000],
        [0.2500, 0.2500, 0.2500, 0.2500]])
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;When &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;wei&lt;/code&gt; is multiplied with any second matrix (of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;(T, ?)&lt;/code&gt; shape), it will perform a weighted average over the vectors from this second matrix.&lt;/p&gt;

&lt;p&gt;Here is an example:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.3107&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;0.2057&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;0.9657&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;0.7057&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;wei&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;which gives&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nf&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.3107&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.0525&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;0.2869&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;0.3916&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;span class=&quot;small-txt&quot;&gt;* where, for instance &lt;/span&gt;&lt;code class=&quot;language-plaintext small-txt highlighter-rouge&quot;&gt;-0.0525 = -0.3107 * 0.5 + 0.2057 * 0.5 + 0.9657 * 0.0 + 0.7057 * 0.0&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Masks don’t need to be causal, and are actually useful in many different situations; that’s what we’re going to explore in the next section.&lt;/p&gt;

&lt;h2 id=&quot;masking-for-self-supervised-learning&quot;&gt;Masking for self-supervised learning&lt;/h2&gt;

&lt;p&gt;Going further with applying successful designs from LLM to RL, the concepts of &lt;strong&gt;pre-training and fine-tuning&lt;/strong&gt; were introduced. The idea is to &lt;strong&gt;decouple representation learning from policy learning&lt;/strong&gt;, using the pre-training step to create a first generative model, later fine-tuning it for specific tasks.&lt;/p&gt;

&lt;p&gt;The pre-training step is unsupervised or self-supervised, which makes an important difference with DT, which used returns for supervision. It is where the model will learn useful representation of the state transition function and of the system dynamics, while the fine-tuning step aims to build the actual control policy.&lt;/p&gt;

&lt;p&gt;In the next sections, we’ll dive into how efficient pre-training is achieved with self-supervised learning. Again, masks are an interesting ingredient to create control-centric objectives that help learn how the environment behaves.&lt;/p&gt;

&lt;h3 id=&quot;decoupling-representation-learning-from-policy-learning&quot;&gt;Decoupling Representation Learning from Policy Learning&lt;/h3&gt;

&lt;p&gt;Several studies demonstrated how representation learning, that is learning a useful representation of the environment (a “latent state”) that contains significant information on the original state and ignores the rest, can dramatically improve performance in a variety of tasks.&lt;/p&gt;

&lt;p&gt;“&lt;i&gt;Decoupling Representation Learning from Reinforcement Learning&lt;/i&gt;” &lt;a href=&quot;#9&quot;&gt;[9]&lt;/a&gt; defines an unsupervised learning task that requires the model to associate an observation with one from a specified, near-future time step.&lt;/p&gt;

&lt;p&gt;In “&lt;i&gt;Guaranteed Discovery of Control-Endogenous Latent States with Multi-Step Inverse Models&lt;/i&gt;” &lt;a href=&quot;#10&quot;&gt;[10]&lt;/a&gt;, authors introduce another unsupersived method that consists in predicting actions from observations to discover a latent representation of the system.&lt;/p&gt;

&lt;p&gt;In “&lt;i&gt;Which Mutual-Information Representation Learning Objectives are Sufficient for Control?&lt;/i&gt;” &lt;a href=&quot;#11&quot;&gt;[11]&lt;/a&gt;, the study assesses different representation learning methods like “Forward dynamics” and “Inverse dynamics” prediction.&lt;/p&gt;

&lt;p&gt;Finally, “&lt;i&gt;SMART: Self-supervised Multi-task pretrAining with contRol Transformers&lt;/i&gt;” &lt;a href=&quot;#12&quot;&gt;[12]&lt;/a&gt; builds on these techniques and shows how a self-supervised pre-training task formed by 3 sub-objectives can outperform other Decision Transformers models on many challenging environments.&lt;/p&gt;

&lt;p&gt;Let’s see some of these representation learning techniques, that rely on masking.&lt;/p&gt;

&lt;h4 id=&quot;inverse-dynamics-prediction&quot;&gt;Inverse dynamics prediction&lt;/h4&gt;

&lt;p&gt;&lt;img src=&quot;/assets/masking/inverse_dyn.svg&quot; alt=&quot;sources&quot; class=&quot;double-left&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In Inverse dynamics prediction, the goal is to recover what action $a_t$ lead from $o_t$ to $o_{t+1}$. The &lt;strong&gt;learned representation of observations will then contain useful information to predict actions, discarding those that are irrelevant&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;To do that, we need take a lower triangular matrix and mask out the selected elements corresponding to action at $t$ step under the diagonal set to 0. Here is how it should look like.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;prettyprint&quot;&gt;&lt;b&gt;o1 a1 o2 a2 ...&lt;/b&gt;
1  0  0  0  0
1  1  0  0  0
1  &lt;span style=&quot;background-color:yellow&quot;&gt;0&lt;/span&gt;  1  0  0
1  1  1  1  0
1  1  1  &lt;span style=&quot;background-color:yellow&quot;&gt;0&lt;/span&gt;  1&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Using Python, here is an implementation&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;itertools&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# our context length
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# positions where to start masking
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;start&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;rm_idx&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# initial lower triangular matrix
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mask&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;tril&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;ones&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# compute indexes of a(t) to mask out
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ind_x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;start&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ind_y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rm_idx&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ind_xy&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;j&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;itertools&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;product&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ind_x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ind_y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)])&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# apply on mask
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mask&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ind_xy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ind_xy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;gives&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;prettyprint&quot;&gt;tensor([[1., 0., 0., 0., 0., 0., 0., 0.],
        [1., 1., 0., 0., 0., 0., 0., 0.],
        [1., &lt;span style=&quot;background-color:yellow&quot;&gt;0.&lt;/span&gt;, 1., 0., 0., 0., 0., 0.],
        [1., 1., 1., 1., 0., 0., 0., 0.],
        [1., 1., 1., &lt;span style=&quot;background-color:yellow&quot;&gt;0.&lt;/span&gt;, 1., 0., 0., 0.],
        [1., 1., 1., 1., 1., 1., 0., 0.],
        [1., 1., 1., 1., 1., &lt;span style=&quot;background-color:yellow&quot;&gt;0.&lt;/span&gt;, 1., 0.],
        [1., 1., 1., 1., 1., 1., 1., 1.]])
&lt;/code&gt;&lt;/pre&gt;

&lt;!--

#### Dealing with multiple tokens for a single observation

Sometimes there is more than one token for a single observation. Let's say you have images divided into 2 patches, denoted as $P_1$ and $P_2$, the ideal mask would be:

&lt;pre&gt;&lt;code class=&quot;prettyprint&quot;&gt;&lt;b&gt;   P1 P2  a P1 P2&lt;/b&gt; ...
&lt;b&gt;P1&lt;/b&gt;  1  &lt;span style=&quot;background-color:yellow&quot;&gt;1&lt;/span&gt;  0  0  0
&lt;b&gt;P2&lt;/b&gt;  1  1  0  0  0
&lt;b&gt;a &lt;/b&gt;  1  1  1  0  0
&lt;b&gt;P1&lt;/b&gt;  1  1  1  1  &lt;span style=&quot;background-color:yellow&quot;&gt;1&lt;/span&gt;
&lt;b&gt;P2&lt;/b&gt;  1  1  1  1  1  
...&lt;/code&gt;&lt;/pre&gt;

Higlights are parts of the observation that need to be preserved, as we can't mask out some patches from others. In this scenario, the default causal mask wouldn't fit well the purpose.

Here is a sample implementation:

```python
T = 6
# number of obs, action and reward tokens
tokens_per_step = 3
# number of obs tokes, should be &gt; 1 for the given example
num_obs_tokens = 2
# the initial causal mask (lower triangular matrix)
causal_mask = torch.tril(torch.ones((T, T)))

num_timesteps = T // tokens_per_step
num_non_obs_tokens = tokens_per_step - num_obs_tokens
diag = [
  torch.ones((num_obs_tokens, num_obs_tokens)) 
  if i % 2 == 0 
  else torch.zeros((num_non_obs_tokens, num_non_obs_tokens))
  for i in range(num_timesteps * 2)
]
block_diag = torch.block_diag(*diag)
custom_mask = torch.add(causal_mask, block_diag).bool().float()
``` 

gives

&lt;pre&gt;&lt;code class=&quot;prettyprint&quot;&gt;tensor([[1., &lt;span style=&quot;background-color:yellow&quot;&gt;1.&lt;/span&gt;, 0., 0., 0., 0.],
        [1., 1., 0., 0., 0., 0.],
        [1., 1., 1., 0., 0., 0.],
        [1., 1., 1., 1., &lt;span style=&quot;background-color:yellow&quot;&gt;1.&lt;/span&gt;, 0.],
        [1., 1., 1., 1., 1., 0.],
        [1., 1., 1., 1., 1., 1.]])&lt;/code&gt;&lt;/pre&gt;

--&gt;

&lt;h4 id=&quot;random-trajectory-masking&quot;&gt;Random trajectory masking&lt;/h4&gt;

&lt;p&gt;&lt;img src=&quot;/assets/masking/rand_traj_pred.svg&quot; alt=&quot;sources&quot; class=&quot;double-left&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Instead of using a separate mask, it is also possible to mask out tokens directly. This is an effective technique in generative tasks like text and images generation.&lt;/p&gt;

&lt;p&gt;For instance, in “&lt;i&gt;BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding&lt;/i&gt;” &lt;a href=&quot;#6&quot;&gt;[6]&lt;/a&gt; authors randomly replace some tokens with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;[MASK]&lt;/code&gt; or a random word. As stated in the paper:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;The advantage of this procedure is that the Transformer encoder does not know which words it will be asked to predict or which have been replaced by random words, so it is forced to keep a distributional contextual representation of every input token.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&quot;/assets/masking/mae.png&quot; alt=&quot;sources&quot; class=&quot;double-right-m&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In the image processing field, a good example comes from “&lt;i&gt;Masked Autoencoders Are Scalable Vision Learners&lt;/i&gt;” &lt;a href=&quot;#7&quot;&gt;[7]&lt;/a&gt;. It shows how an efficient masking strategy using grayed out image patches can bring both greater generalization capabilites and better scalability.&lt;/p&gt;

&lt;p&gt;Figure on the right comes from the paper and shows image reconstruction capabilities using different masking ratios.&lt;/p&gt;

&lt;div style=&quot;margin-top: 3rem;&quot;&gt;&lt;/div&gt;

&lt;p&gt;These techniques can also be used in control-related problems. The intuition would be to hide part of a trajectory (actions, observations), so that the model learns to reconstruct it. Again, &lt;strong&gt;the model would learn system dynamics by guessing what action(s) and/or observation(s) led to a specific state&lt;/strong&gt;. Obsiously, the difficulty increases with the size of the mask.&lt;/p&gt;

&lt;p&gt;The main difference with “Inverse dynamics prediction” here will stand in the mask itself: instead of predicting a single action while hiding the future, we’ll let the model see it and predict the whole trajectory (or part of it). While the 2 technics seem pretty similar, their addition demonstrates benefits in model performance.&lt;/p&gt;

&lt;p&gt;Let’s take a simple example where we want to mask the first 2 action tokens in a trajectory. Here is how the tokens should look like after masking them, given that &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-1&lt;/code&gt; is used as a masking value.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;prettyprint&quot;&gt;&lt;b&gt;o1 a1 o2  a2 o3 a3&lt;/b&gt;...
 1 &lt;span style=&quot;background-color:yellow&quot;&gt;-1&lt;/span&gt;  1  &lt;span style=&quot;background-color:yellow&quot;&gt;-1&lt;/span&gt;  1  1
 1 &lt;span style=&quot;background-color:yellow&quot;&gt;-1&lt;/span&gt;  1  &lt;span style=&quot;background-color:yellow&quot;&gt;-1&lt;/span&gt;  1  1
 1 &lt;span style=&quot;background-color:yellow&quot;&gt;-1&lt;/span&gt;  1  &lt;span style=&quot;background-color:yellow&quot;&gt;-1&lt;/span&gt;  1  1
 1 &lt;span style=&quot;background-color:yellow&quot;&gt;-1&lt;/span&gt;  1  &lt;span style=&quot;background-color:yellow&quot;&gt;-1&lt;/span&gt;  1  1&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The implementation is quite straightfoward.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# our dimensions
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;B&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;C&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# tokens that will be partially masked
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;masked_tokens&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;ones&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;B&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;C&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# the index of vectors, here actions, that will be masked
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mask_idx&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# masked token value
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;emb_mask_value&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;j&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mask_idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)):&lt;/span&gt;
  &lt;span class=&quot;c1&quot;&gt;# actions are at odd indices
&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;masked_tokens&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mask_idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;emb_mask_value&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Result (reduced to 2 dimensions for the sake of clarity):&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;prettyprint&quot;&gt;tensor([[ 1., &lt;span style=&quot;background-color:yellow&quot;&gt;-1.&lt;/span&gt;,  1., &lt;span style=&quot;background-color:yellow&quot;&gt;-1.&lt;/span&gt;,  1.,  1.,  1.,  1.],
        [ 1., &lt;span style=&quot;background-color:yellow&quot;&gt;-1.&lt;/span&gt;,  1., &lt;span style=&quot;background-color:yellow&quot;&gt;-1.&lt;/span&gt;,  1.,  1.,  1.,  1.],
        [ 1., &lt;span style=&quot;background-color:yellow&quot;&gt;-1.&lt;/span&gt;,  1., &lt;span style=&quot;background-color:yellow&quot;&gt;-1.&lt;/span&gt;,  1.,  1.,  1.,  1.],
        [ 1., &lt;span style=&quot;background-color:yellow&quot;&gt;-1.&lt;/span&gt;,  1., &lt;span style=&quot;background-color:yellow&quot;&gt;-1.&lt;/span&gt;,  1.,  1.,  1.,  1.]])&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;closing-the-loop-loss-computation&quot;&gt;Closing the loop: loss computation&lt;/h3&gt;

&lt;p&gt;What’s next? Once we have our masked tokens, we’ll want to predict what we held out and compute our loss against actual targets.&lt;/p&gt;

&lt;p&gt;Example below builds on previous “random trajectory masking” output and compute such loss.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch.nn&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;functional&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;F&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# this is a pass-all-through mask (only ones), so attention blocks can see in the future
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;noop_mask&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;ones&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;B&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;C&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# forward pass on multi-head attention blocks (not define here for brevity)
# masked_tokens is defined in previous example
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;attn_blocks&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;masked_tokens&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;noop_mask&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# forward pass on a prediction head, typically a set of linear layers
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;logits&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;pred_head&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mask_idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:])&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# actual targets
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;action_targets&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;actions&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mask_idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:]&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# loss computation. MSE used for continuous actions, cross entropy for discrete ones
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;F&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;mse_loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;logits&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;action_targets&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;If we have multiple pre-training objectives, we can simply add up the losses together and optimize a single loss, for a complex and informative control-oriented pre-training task.&lt;/p&gt;

&lt;h2 id=&quot;conclusion-and-opportunities&quot;&gt;Conclusion and opportunities&lt;/h2&gt;

&lt;p&gt;In this article we’ve seen how masking can be used for representation learning in self-supervised pre-training. Through simple masking examples, we’ve seen how a model can learn how a system behaves. Masking is a versatile technique, and designing expressive and system-specific objectives should be possible using this technique.&lt;/p&gt;

&lt;p&gt;These concepts, inherited from the NLP and vision fields, are certainly key to create &lt;strong&gt;decision foundation models&lt;/strong&gt;, which learn important dynamics of the systems at hand and can produce accurate trajectories for modeling downstream tasks like control policies. With robust foundation models, it would then be possible to train policies using a variety techniques, including but not limited to RL.  Imitation learning (IL) or Model Predictive Control (MPC) can as well take advantage of the generative nature of the pre-trained model.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;p&gt;&lt;a id=&quot;1&quot;&gt;[1]&lt;/a&gt;
Huang, Shengyi and Ontañón, Santiago.
&lt;i&gt;A Closer Look at Invalid Action Masking in Policy Gradient Algorithms. &lt;/i&gt;&lt;a href=&quot;https://arxiv.org/abs/2006.14171&quot;&gt;2022&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a id=&quot;2&quot;&gt;[2]&lt;/a&gt;
Adil Zouitine.
&lt;i&gt;Masking in Deep Reinforcement Learning. &lt;/i&gt;&lt;a href=&quot;https://boring-guy.sh/posts/masking-rl/&quot;&gt;2022&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a id=&quot;3&quot;&gt;[3]&lt;/a&gt;
Emilio Parisotto and H. Francis Song and Jack W. Rae and Razvan Pascanu and Caglar Gulcehre and Siddhant M. Jayakumar and Max Jaderberg and Raphael Lopez Kaufman and Aidan Clark and Seb Noury and Matthew M. Botvinick and Nicolas Heess and Raia Hadsell.
&lt;i&gt;Stabilizing Transformers for Reinforcement Learning. &lt;/i&gt;&lt;a href=&quot;https://arxiv.org/abs/1910.06764&quot;&gt;2019&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a id=&quot;4&quot;&gt;[4]&lt;/a&gt;
Tianwei Ni and Michel Ma and Benjamin Eysenbach and Pierre-Luc Bacon.
&lt;i&gt;When Do Transformers Shine in RL? Decoupling Memory from Credit Assignment. &lt;/i&gt;&lt;a href=&quot;https://arxiv.org/abs/2307.03864&quot;&gt;2023&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a id=&quot;5&quot;&gt;[5]&lt;/a&gt;
Lili Chen and Kevin Lu and Aravind Rajeswaran and Kimin Lee and Aditya Grover and Michael Laskin and Pieter Abbeel and Aravind Srinivas and Igor Mordatch.
&lt;i&gt;Decision Transformer: Reinforcement Learning via Sequence Modeling. &lt;/i&gt;&lt;a href=&quot;https://arxiv.org/abs/2106.01345&quot;&gt;2021&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a id=&quot;6&quot;&gt;[6]&lt;/a&gt;
Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova.
&lt;i&gt;BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. &lt;/i&gt;&lt;a href=&quot;https://arxiv.org/abs/1810.04805&quot;&gt;2019&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a id=&quot;7&quot;&gt;[7]&lt;/a&gt;
Kaiming He and Xinlei Chen and Saining Xie and Yanghao Li and Piotr Dollár and Ross Girshick
&lt;i&gt;Masked Autoencoders Are Scalable Vision Learners. &lt;/i&gt;&lt;a href=&quot;https://arxiv.org/abs/2111.06377&quot;&gt;2021&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a id=&quot;8&quot;&gt;[8]&lt;/a&gt;
Keiran Paster and Sheila McIlraith and Jimmy Ba
&lt;i&gt;You Can’t Count on Luck: Why Decision Transformers and RvS Fail in Stochastic Environments. &lt;/i&gt;&lt;a href=&quot;https://arxiv.org/abs/2205.15967&quot;&gt;2022&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a id=&quot;9&quot;&gt;[9]&lt;/a&gt;
Adam Stooke and Kimin Lee and Pieter Abbeel and Michael Laskin.
&lt;i&gt;Decoupling Representation Learning from Reinforcement Learning. &lt;/i&gt;&lt;a href=&quot;https://arxiv.org/abs/2009.08319&quot;&gt;2021&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a id=&quot;10&quot;&gt;[10]&lt;/a&gt;
Alex Lamb and Riashat Islam and Yonathan Efroni and Aniket Didolkar and Dipendra Misra and Dylan Foster and Lekan Molu and Rajan Chari and Akshay Krishnamurthy and John Langford.
&lt;i&gt;Guaranteed Discovery of Control-Endogenous Latent States with Multi-Step Inverse Models. &lt;/i&gt;&lt;a href=&quot;https://arxiv.org/abs/2207.08229&quot;&gt;2022-arxiv&lt;/a&gt; &lt;a href=&quot;https://controllable-latent-state.github.io/&quot;&gt;2022-blog&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a id=&quot;11&quot;&gt;[11]&lt;/a&gt;
Kate Rakelly and Abhishek Gupta and Carlos Florensa and Sergey Levine.
&lt;i&gt;Which Mutual-Information Representation Learning Objectives are Sufficient for Control?. &lt;/i&gt;&lt;a href=&quot;https://arxiv.org/abs/2106.07278&quot;&gt;2021&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a id=&quot;12&quot;&gt;[12]&lt;/a&gt;
Yanchao Sun and Shuang Ma and Ratnesh Madaan and Rogerio Bonatti and Furong Huang and Ashish Kapoor.
&lt;i&gt;SMART: Self-supervised Multi-task pretrAining with contRol Transformers. &lt;/i&gt;&lt;a href=&quot;https://arxiv.org/abs/2301.09816&quot;&gt;2023&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a id=&quot;13&quot;&gt;[13]&lt;/a&gt;
Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin.
&lt;i&gt;Attention Is All You Need. &lt;/i&gt;&lt;a href=&quot;https://arxiv.org/abs/1706.03762&quot;&gt;2017&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a id=&quot;14&quot;&gt;[14]&lt;/a&gt;
Andrej Karpathy.
&lt;i&gt;Let’s build GPT: from scratch, in code, spelled out. &lt;/i&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=kCc8FmEb1nY&quot;&gt;2023&lt;/a&gt;&lt;/p&gt;

&lt;script src=&quot;https://cdn.jsdelivr.net/gh/google/code-prettify@master/loader/run_prettify.js&quot;&gt;&lt;/script&gt;

</description>
        <pubDate>Wed, 15 Nov 2023 01:00:00 +0100</pubDate>
        <link>http://localhost:4000/ai/deeplearning/transformers/mask/masking.html</link>
        <guid isPermaLink="true">http://localhost:4000/ai/deeplearning/transformers/mask/masking.html</guid>
        
        
        <category>ai</category>
        
        <category>deeplearning</category>
        
        <category>transformers</category>
        
        <category>mask</category>
        
      </item>
    
      <item>
        <title>Using Physics-informed Neural Networks to solve 2D heat equation</title>
        <description>
&lt;style type=&quot;text/css&quot;&gt;
.center {
    display:block;
    margin: 0 auto;
}
.double-left {
    width:49%;
    display:block;
    float:left;
    margin: 0 auto;
    margin-right: 10px;
}
.add-margin-right {
    margin-right: 20px;
}
.double {
    width: 49%;
    margin: 0 auto;
}
.double-unconst {
    width: auto;
    margin: 0 auto;
    margin-left: 1.5rem;
}
.image-foot {
    font-size:10pt;
    max-width: 30rem;
    margin: 0 auto;
}
.small-img {
  width: 45%;
}
ul {
  display: table;
}
&lt;/style&gt;

&lt;p&gt;In real world problems, data is often scarce and noisy, limiting how fast an ML/AI-based solution can become effective 
in the field. Mathematical models of physical phenomena exist, but they require intractable computation times and level of
details about the environment.&lt;/p&gt;

&lt;p&gt;Physics-informed neural networks (PINNs) were recently introduced as an elegant and efficient way to mary the best of both worlds:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;learn from the theoretical model represented by a partial differential equation (PDE), to ensure robust predictions and limit amount of field data to collect&lt;/li&gt;
  &lt;li&gt;learn from field data to adapt the trained neural network to the target environment&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This article deals with how well a PINN can solve the well-known heat equation and shows that it could become an enabler for many real-world applications that are currently too complex to solve with deep learning.&lt;/p&gt;

&lt;h2 id=&quot;pinns-an-overview&quot;&gt;PINNs, an overview&lt;/h2&gt;

&lt;p&gt;Many physical aspects of our surrounding world can be described using PDEs. But computing the solution to these equations 
often require complex models and details about the environment that are often not available. Worse, solutions would need 
to be recomputed for every change in the PDE conditions, which will always happen. Depending on the variability of the 
environment, the number of computations would grow exponentially.&lt;/p&gt;

&lt;p&gt;Physics-informed neural networks &lt;a href=&quot;#1&quot;&gt;[1]&lt;/a&gt;&lt;a href=&quot;#2&quot;&gt;[2]&lt;/a&gt; offer an elegant and flexible solution to solve PDEs in a more universal manner:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;thanks to the capacity of neural networks to approximate arbitrary functions, they can also approximate PDEs involving derivatives of different orders&lt;/li&gt;
  &lt;li&gt;they are not limited by discretization like classic approximators of PDEs, thus can infer any point in space&lt;/li&gt;
  &lt;li&gt;they are not limited to specific boundary or initial conditions, given that they are appropriately trained (ie conditions given as input)&lt;/li&gt;
  &lt;li&gt;they take advantage of modern deep learning frameworks auto differentiation methods to compute derivatives (ie &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.autograd&lt;/code&gt;)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In this article we’ll see how we can solve the 2 dimensional heat equation.&lt;/p&gt;

&lt;h2 id=&quot;the-heat-equation-a-partial-differential-equation&quot;&gt;The Heat Equation, a Partial Differential Equation&lt;/h2&gt;

&lt;p&gt;Heat diffusion equation &lt;a href=&quot;#3&quot;&gt;[3]&lt;/a&gt; describes the diffusion of heat over time and space. It’s a PDE, involving time and space 
derivatives. The basic equation in a 2D space is:&lt;/p&gt;

&lt;center&gt;
$$ 
\begin{equation}
\frac{\partial u}{\partial t} = \alpha \frac{\partial^2 u}{\partial x} \frac{\partial^2 u}{\partial y}
\end{equation}
$$
&lt;/center&gt;

&lt;p&gt;Note: it’s often simplified using notation $ u_t = \alpha \cdot u_{xx} \cdot u_{yy} $&lt;/p&gt;

&lt;p&gt;where $u$ is the function that associates $t, x, y$ with a temperature, $x$ and $y$ points on the 2D surface, $t$ the time 
variable and $\alpha$ the coefficient of diffusion.&lt;/p&gt;

&lt;p&gt;A PDE is also defined by its boundary and initial conditions. In present case we’ll set left, bottom and right edges as 
cold surfaces at 0 unit of temperature, while the top is heated at 1 unit of temperature:&lt;/p&gt;

&lt;center&gt;
$$
u(t, 0, y) = u(t, x_{max}, y) = u(t, x, 0) = 0, \text{with} \space 0 \leq y &amp;lt; y_{max}
$$
$$
u(t, x, y_{max}) = 1
$$
&lt;/center&gt;

&lt;p&gt;As an initial condition, we’ll consider the whole surface at 0 unit of temperature, ie $u(0, x, y) = 0$.&lt;/p&gt;

&lt;h2 id=&quot;solving-with-finite-difference-method&quot;&gt;Solving with Finite-Difference Method&lt;/h2&gt;

&lt;p&gt;The Finite-Difference Method (FDM) offers a way to numerically solve PDEs using discrete space and time spaces 
to enable derivative functions approximation that we saw in $(1)$.&lt;/p&gt;

&lt;p&gt;First and second order derivatives can be approximated with&lt;/p&gt;

&lt;center&gt;
$$
\begin{equation}
\frac{\partial u}{\partial t} \approx \frac{u_{i,j}^{k+1} - u_{i,j}^{k}}{\Delta t} \\
\frac{\partial^2 u}{\partial x} = \frac{\partial}{\partial x} \cdot (\frac{\partial u}{\partial x}) \approx \frac{u_{i+1,j}^{k} - 2u_{i,j}^{k} + u_{i-1,j}^{k}}{\Delta x^2}
\end{equation}
$$ 
&lt;/center&gt;

&lt;p&gt;If we transform the whole original equation $(1)$ using $(2)$, we get:&lt;/p&gt;

&lt;center&gt;
$$
\begin{equation}
\frac{u_{i,j}^{k+1} - u_{i,j}^{k}}{\Delta t} = \alpha (\frac{u_{i+1,j}^{k} - 2u_{i,j}^{k} + u_{i-1,j}^{k}}{\Delta x^2} + \frac{u_{i,j+1}^{k} - 2u_{i,j}^{k} + u_{i,j-1}^{k}}{\Delta y^2})
\end{equation}
$$
&lt;/center&gt;

&lt;p&gt;Taking $\Delta x = \Delta y$ (a grid with steps of equal size in all directions), we can simplify the equation to:&lt;/p&gt;

&lt;center&gt;
$$
\begin{equation}
u_{i,j}^{k+1} = \gamma(u_{i+1,j}^{k} + u_{i-1,j}^{k} + u_{i,j+1}^{k} + u_{i,j-1}^{k} -4u_{i,j}^{k}) + u_{i,j}^{k} \\
\text{with} \space \gamma = \alpha \frac{\Delta t}{\Delta x^2}
\end{equation}
$$
&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;br /&gt;
$(4)$ can be quickly implemented using a 3 nested for loops in python, or you can go for a more efficient method using 
Jax and its Just In Time (JIT) compiler:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;delta_t&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.01&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;delta_x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;gamma&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;delta_t&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;delta_x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;**&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;

&lt;span class=&quot;nd&quot;&gt;@jax.jit&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;compute_next_k&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;u_k&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;next_u&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;jnp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;u_k&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;u_next_k&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gamma&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;jnp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;roll&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;u_k&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;jnp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;roll&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;u_k&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;jnp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;roll&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;u_k&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;jnp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;roll&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;u_k&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
            &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;u_k&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;u_k&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;next_u&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;at&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;set&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;u_next_k&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;solving-with-pinn&quot;&gt;Solving with PINN&lt;/h2&gt;

&lt;p&gt;Using PINN, we don’t explicitly solve the equation anymore, but it will be used to compute the loss during the training 
of the neural network. Specifically, the loss will be computed against the derivative of the PDE, and the boundary and initial 
conditions. In our present case, our loss $L_{eq}$ can be defined to minimize residual of the predicted heat equation $(1)$&lt;/p&gt;

&lt;p&gt;Given that we want $ u_t - \alpha u_{xx} u_{yy} = 0 $, the loss can be obtained with&lt;/p&gt;

&lt;center&gt;
$$
L_{eq} = \frac{1}{N} \sum_{i=1}^{n} \| u_t^i - \alpha u_{xx}^i u_{yy}^i \|^2
$$
&lt;/center&gt;

&lt;p&gt;Similarly, residuals for boundary (BC) and initial (IC) conditions can be defined as&lt;/p&gt;

&lt;center&gt;
$$
L_{bc} = \frac{1}{N} \sum_{i=1}^{n} \| u(t, x^b, y^b) - u_{bc} \|^2
$$
$$
L_{ic} = \frac{1}{N} \sum_{i=1}^{n} \| u(0, x, y) - u_{ic} \|^2
$$
&lt;/center&gt;

&lt;p&gt;The total loss is then:&lt;/p&gt;

&lt;center&gt;
$$
L_{pde} = L_{eq} + L_{bc} + L_{ic}
$$
&lt;/center&gt;

&lt;p&gt;PINN reveals to be a tractable solution because derivatives computation can be delegated to deep learning frameworks we 
use to train neural networks: auto differentiation technique is already implemented, so we’ll use it for both back-propagation 
but also to compute $L_{pde}$.&lt;/p&gt;

&lt;p&gt;An example of using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pytorch&lt;/code&gt; to compute $L_{eq}$:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch.autograd&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grad&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# where u is the neural net module we train, so here we actually perform a forward pass
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;u_value&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;u&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; 
&lt;span class=&quot;c1&quot;&gt;# diffusion coefficient
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;_grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;inp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
      &lt;span class=&quot;n&quot;&gt;inp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
      &lt;span class=&quot;n&quot;&gt;grad_outputs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;ones_like&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;inp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; 
      &lt;span class=&quot;n&quot;&gt;create_graph&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;)[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;loss_eq&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
  &lt;span class=&quot;nf&quot;&gt;_grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;u_value&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; 
  &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt; 
    &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;_grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;_grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;u_value&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; 
    &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;_grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;_grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;u_value&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;results&quot;&gt;Results&lt;/h2&gt;

&lt;p&gt;We train a small PINN on 2000 epochs and compare visually against FDM method:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/pinn/fdm_solution.gif&quot; alt=&quot;sources&quot; class=&quot;center&quot; /&gt;
&lt;img src=&quot;/assets/pinn/pinn_solution.gif&quot; alt=&quot;sources&quot; class=&quot;center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The PINN solution looks very close to the FDM one, that’s what we wanted!&lt;/p&gt;

&lt;h2 id=&quot;real-world-applications&quot;&gt;Real world applications&lt;/h2&gt;

&lt;p&gt;Given that PDEs can be used to model a lot of problems, there are many possible real-world applications for PINNs. Looking for 
recent papers exploiting them is sufficient to imagine the number of potential new AI-based solutions that may become practical.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/pinn/pinn_papers.png&quot; alt=&quot;sources&quot; class=&quot;center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;With PINNs, it becomes possible to quickly create efficient industrial controllers, starting with minimal amount of field data. When field 
data is available, training loss can be computed as $L_{total} = L_{pde} + L_{data}$. As more and more field data becomes available, 
the weight of $L_{data}$ can be increased so the solution lean towards the specifics of the target environment, 
while $L_{pde}$ acts as a regularization term to make sure predicted solution is still robust with respect to physics.&lt;/p&gt;

&lt;p&gt;This is particularly well-suited for heating and cooling applications in small buildings, where rooms are often treated 
by one terminal unit, like a hot water baseboard: natural convection and radiation mechanisms can be modeled and used to 
train a PINN, which can in turn be used to control the appliance.&lt;/p&gt;

&lt;p&gt;Another point in favor of PINNs is that boundary and initial conditions can be encoded as input. This will allow the 
PINN to generalize to different environments, or contexts for a given environment. This way there would be no need to 
retrain a model for every change in conditions.&lt;/p&gt;

&lt;p&gt;To avoid the heavy-lifting of experimenting and implementing from scratch, some libraries already exist, IDRLnet &lt;a href=&quot;#4&quot;&gt;[4]&lt;/a&gt;. 
It’s well suited for iterative experimentation and already implements common PDEs (Heat, Navier-Stokes, Burgers, Wave, …).&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;p&gt;&lt;a id=&quot;1&quot;&gt;[1]&lt;/a&gt;
Raissi, Maziar and Perdikaris, Paris and Karniadakis, George E.
&lt;i&gt;Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations&lt;/i&gt; &lt;a href=&quot;https://www.sciencedirect.com/science/article/pii/S0021999118307125&quot;&gt;2019&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a id=&quot;2&quot;&gt;[2]&lt;/a&gt;
Raissi, Maziar and Perdikaris, Paris and Karniadakis, George E.
&lt;i&gt;Physics Informed Deep Learning (Part I): Data-driven Solutions of Nonlinear Partial Differential Equations&lt;/i&gt; &lt;a href=&quot;https://arxiv.org/abs/1711.10561&quot;&gt;2017&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a id=&quot;3&quot;&gt;[3]&lt;/a&gt;
&lt;a href=&quot;https://en.wikipedia.org/wiki/Heat_equation&quot;&gt;Heat Equation&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a id=&quot;4&quot;&gt;[4]&lt;/a&gt;
&lt;a href=&quot;https://github.com/idrl-lab/idrlnet&quot;&gt;IDRLnet&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Thu, 15 Sep 2022 02:00:00 +0200</pubDate>
        <link>http://localhost:4000/ai/deeplearning/physics/pinns-heat-equation.html</link>
        <guid isPermaLink="true">http://localhost:4000/ai/deeplearning/physics/pinns-heat-equation.html</guid>
        
        
        <category>ai</category>
        
        <category>deeplearning</category>
        
        <category>physics</category>
        
      </item>
    
      <item>
        <title>Autoregressive control policies for HVAC optimization</title>
        <description>
&lt;style type=&quot;text/css&quot;&gt;
.center {
    display:block;
    margin: 0 auto;
}
.double-left {
    width:49%;
    display:block;
    float:left;
    margin: 0 auto;
    margin-right: 10px;
}
.add-margin-right {
    margin-right: 20px;
}
.double {
    width: 49%;
    margin: 0 auto;
}
.double-unconst {
    width: auto;
    margin: 0 auto;
    margin-left: 1.5rem;
}
.image-foot {
    font-size:10pt;
    max-width: 30rem;
    margin: 0 auto;
}
.small-img {
  width: 45%;
}
ul {
  display: table;
}
&lt;/style&gt;

&lt;p&gt;Real world applications of Reinforcement Learning (RL) are still limited as the technology is brittle and requires an enormous amount of simulated interactions. The power of simulated Heating, Ventilation and Air Conditioning (HVAC) systems represent a unique opportunity to use RL in the real systems. Nonetheless, HVAC represent an array of different challenges such as learning robust policies from simulation that can transfer into the real world and defining a reward function that will lead to the desired behavior. In this article, we examine different strategies to deal with the rich action spaces of HVAC systems.&lt;/p&gt;

&lt;h2 id=&quot;problem-setting&quot;&gt;Problem Setting&lt;/h2&gt;

&lt;p&gt;HVAC systems are complex and made of multiple sub-systems that interact in complex, non-linear ways (see a brief introduction on that in &lt;a href=&quot;https://airboxlab.github.io/hvac/control/ai/reinforcement_learning/2021/01/24/smart_control.html&quot;&gt;HVAC processes control: can AI help?&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;HVAC air and plant loops are responsible for maintaining comfort in buildings occupied zones. Their continuous control is made of a sequence of decisions. Taking the example of an air loop, it can start by deciding how much fresh air the systems lets in, then deciding at what flow rate it must be pushed and at what temperature, and ending with how this air must be “adjusted” for the target zone.
Many equipment are involved, like fans, heating and cooling coils, chilled and hot water plant loops, etc.&lt;/p&gt;

&lt;p&gt;Let’s build our intuition with a simple example: a heater is equipped with a hot water coil and a fan, and heats the space by controlling the hot water valve and the fan to blow the hot air. If you turn the fan off but keep valve open, hot air won’t move much, and energy consumption will be near 0. If you turn it to maximum position or speed, hot air will move quicker, room will heat quicker too, and energy consumption will be high, both because of the higher fan electricity consumption, and because it increases heat exchange between air and hot water coil.&lt;/p&gt;

&lt;p&gt;As a consequence, optimizing control on this simple heating system would require to consider both fan speed and hot water valve position, with prior knowledge that fan and valve have a non-linear dependency regarding temperature of the air blown and energy consumption.&lt;/p&gt;

&lt;p&gt;If we generalize this example, many parts of the HVAC system can be seen as dependent to or having a direct or indirect impact on other parts. Trying to control only one equipment as a whole will necessarily lead to suboptimal performance. However, this is often how HVAC systems are controlled: each equipment takes decision regarding a single input. No matter how complex the calculation of this input is, it won’t be differentiable since it’s a single scalar value. Common calculation methods for control decisions rely on linear methods (linear interpolations, PIDs, …) with a narrow view of the state of the system.&lt;/p&gt;

&lt;p&gt;That’s where autoregressive (AR) methods and deep reinforcement learning (DRL) can be a good fit: combining function approximation capabilities of neural networks, theoritically unlimited observability of the system and modeling of actions dependencies, the HVAC system and the building it serves could be controlled as a whole (holistic approach), and learned policies could model complex relationships between states and equipment.A&lt;/p&gt;

&lt;h2 id=&quot;high-dimensional-action-spaces&quot;&gt;High dimensional action spaces&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/ar_policy/ar_def.svg&quot; alt=&quot;sources&quot; class=&quot;center&quot; /&gt;&lt;/p&gt;
&lt;center class=&quot;image-foot&quot;&gt;&lt;i&gt;fig. 1: 3 ways of modeling decision processes with state / action transitions using independent actions distributions (a.) and AR models (b. and c.)&lt;/i&gt;&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;state-conditionally-independent-actions&quot;&gt;State conditionally independent actions&lt;/h3&gt;

&lt;p&gt;This is the most common case. In many classic problems, each action of a multi-decision problem is modeled and its probability distribution is estimated as independent from other actions. This doesn’t allow to account for relationships between actions that can be encountered in many problems.&lt;/p&gt;

&lt;h3 id=&quot;introducing-action-dependence-with-autoregressive-policies&quot;&gt;Introducing action dependence with autoregressive policies&lt;/h3&gt;
&lt;p&gt;Given a policy &lt;img src=&quot;https://render.githubusercontent.com/render/math?math=\pi_{\theta}&quot; /&gt; to optimize with regards to parameters &lt;img src=&quot;https://render.githubusercontent.com/render/math?math=\theta&quot; /&gt;, state &lt;img src=&quot;https://render.githubusercontent.com/render/math?math=s \in \mathcal{S}&quot; /&gt;, a set of n actions &lt;img src=&quot;https://render.githubusercontent.com/render/math?math=a_{i} \in \mathcal{A}&quot; /&gt;, we can represent the policy as following:&lt;/p&gt;
&lt;center&gt;&lt;img src=&quot;https://render.githubusercontent.com/render/math?math=\pi_{\theta}(a, s) = \prod_{i=1}^{n} \pi_{\theta}(a_{i} \bigm\lvert a_{k&amp;lt;i}, s)&quot; /&gt;&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;This simple formulation is generic and describes simple relationships and order with action at rank i being dependent on all previous actions.&lt;/p&gt;

&lt;p&gt;An example neural network architecture illustrating this formulation is given below&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/ar_policy/ar_nn_arch.svg&quot; alt=&quot;sources&quot; class=&quot;center&quot; /&gt;&lt;/p&gt;
&lt;center class=&quot;image-foot&quot;&gt;&lt;i&gt;fig. 2: example neural network architecture to model conditional dependencies between actions&lt;/i&gt;&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;In this model, a policy with 2 actions is used. It should be noted that:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Action 1&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Action 2&lt;/code&gt; are actually parameters of their respective probability distributions (PD).&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Action 1&lt;/code&gt; scalar value (sampled from estimated PD) is passed as input for &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Action 2&lt;/code&gt; distribution estimation&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Action 1&lt;/code&gt; &lt;em&gt;logits&lt;/em&gt; could be passed as input instead of scalar value. This can help preserve information on large discrete/categorical distributions.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Action 2&lt;/code&gt; inputs could be solely made of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Action 1&lt;/code&gt; output (scalar or logits). In such case policy can be formulated as &lt;img src=&quot;https://render.githubusercontent.com/render/math?math=\pi_{\theta}(a, s) = \pi_{\theta}(a_1, s) \cdot \prod_{i=2}^{n} \pi_{\theta}(a_{i} \bigm\lvert a_{k&amp;lt;i})&quot; /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Note that this model wouldn’t scale well for the estimation of a large number of action distributions. For large action spaces, one can refer to the &lt;em&gt;MADE&lt;/em&gt; architecture &lt;a href=&quot;#1&quot;&gt;[1]&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;experiments&quot;&gt;Experiments&lt;/h2&gt;

&lt;p&gt;To validate benefits of such architecture, we compare performance of “classic” network models with the one described in previous section. To draw a fair comparison, the same total number of neural network cells is used in classic and AR policies networks. Furthermore, all other parameters of the Proximal Policy Optimization (PPO, Shulman et al. &lt;a href=&quot;#2&quot;&gt;[2]&lt;/a&gt;) algorithm are kept equal between the 2 models.&lt;/p&gt;

&lt;h3 id=&quot;a-path-finding-task&quot;&gt;A path-finding task&lt;/h3&gt;

&lt;p&gt;The first task is a toy problem where an agent must navigate through a 2D-grid and maximize its reward by moving to certain locations. The action space is designed with 2 actions (vertical and horizontal movements). 
To emphasize on the need to learn correlation between actions, the reward scheme is modeled using 2 gaussian distributions located on 2 corners of the grid, and a one-time bonus whenever the agent reaches a point close to the mean of each gaussian. The agent starts each episode at the center of the grid. Episode ends when all bonuses are found. We refer to this training environment as bi-modal environment (BME).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/ar_policy/bi_modal_env_grid.png&quot; alt=&quot;sources&quot; class=&quot;center&quot; /&gt;&lt;/p&gt;
&lt;center class=&quot;image-foot&quot;&gt;&lt;i&gt;fig. 3: rewards distribution over the BME grid&lt;/i&gt;&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;We run 5 experiments with different seeds for each method (classic and AR) on the BME and compare the mean return per episode below.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/ar_policy/bi_modal_avg_return.png&quot; alt=&quot;sources&quot; class=&quot;center&quot; /&gt;&lt;/p&gt;
&lt;center class=&quot;image-foot&quot;&gt;&lt;i&gt;fig. 4: average episode reward for non AR policy (classic, light green) and AR policy (dark green)&lt;/i&gt;&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;On this toy environment, AR policy achieves a much better mean episode reward than its counterpart.&lt;/p&gt;

&lt;p&gt;Visual inspection of trajectories for each method can highlight non-efficiencies and differencies. 
Below animations capture sample trajectories after 50 training iterations.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/ar_policy/bi_modal_classic_traj.gif&quot; alt=&quot;sources&quot; class=&quot;double&quot; /&gt;
&lt;img src=&quot;/assets/ar_policy/bi_modal_ar_traj.gif&quot; alt=&quot;sources&quot; class=&quot;double&quot; /&gt;&lt;/p&gt;
&lt;center class=&quot;image-foot&quot;&gt;&lt;i&gt;fig. 5a and 5b: trajectory examples after 50 training iterations of an agent trained under a classic policy (left) and an AR policy (right)&lt;/i&gt;&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;a-simulated-hvac&quot;&gt;A simulated HVAC&lt;/h3&gt;

&lt;p&gt;We now use a simulated building and its HVAC. The building model, or digital twin, is created from an actual building and calibrated on Foobot platform using OpenStudio and EnergyPlus open source software. It is a 3 storey office building, with a dedicated outdoor air system, a single air handling unit (AHU) with eletric heating and chilled water coil served by a local plant, and fan coil units for zones comfort.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/ar_policy/dt_3d_geo.png&quot; alt=&quot;sources&quot; class=&quot;center small-img&quot; /&gt;&lt;/p&gt;
&lt;center class=&quot;image-foot&quot;&gt;&lt;i&gt;fig. 6: office building used as a test environment&lt;/i&gt;&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;A policy is trained with and without autoregressive (AR) actions model. The observation and action spaces, reward function, and PPO parameters are identical between the 2 methods. The action space is composed of 2 actions that command the amount and temperature of air supplied to the zones by the AHU. Each experiment is run for 1e6 timesteps.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/ar_policy/hvac_env_rew.svg&quot; alt=&quot;sources&quot; class=&quot;double&quot; /&gt;
&lt;img src=&quot;/assets/ar_policy/hvac_env_power.svg&quot; alt=&quot;sources&quot; class=&quot;double&quot; /&gt;
&lt;br /&gt;
&lt;img src=&quot;/assets/ar_policy/hvac_env_tmp.svg&quot; alt=&quot;sources&quot; class=&quot;double&quot; /&gt;
&lt;img src=&quot;/assets/ar_policy/hvac_env_co2.svg&quot; alt=&quot;sources&quot; class=&quot;double&quot; /&gt;&lt;/p&gt;
&lt;center class=&quot;image-foot&quot;&gt;&lt;i&gt;fig. 7 average episode reward (a., upper left), power demand (b., upper right, in kW), indoor temperature (c., lower left, in Celsius degrees) and CO2 (d., in ppm) for classic and AR policy (blue line) models&lt;/i&gt;&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;AR policy model not only achieves a better mean episode reward (fig 7a.), but it significantly outperforms (by 8.5%) the energy consumption reduction of the classic method. Other reward components, like thermal comfort and indoor air quality, are also better in line with their respective constraints which are a maximum of 25°C indoor temperature and a maximum of 1000ppm for indoor CO2.&lt;/p&gt;

&lt;h3 id=&quot;analysing-impacts-and-relationships&quot;&gt;Analysing impacts and relationships&lt;/h3&gt;

&lt;p&gt;To understand better our model, a short study is conducted to highlight how action 1 influence action 2, how each action influence the state space, and what part of the state space has the most influence on agent decisions.&lt;/p&gt;

&lt;p&gt;To do that, we’ll first map our system and learned policy with functions approximations that govern it. They can be seen as forward models that predict the behavior of our policy.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;transitions&lt;/em&gt; that associates an action and a state to a next state &lt;img src=&quot;https://render.githubusercontent.com/render/math?math=f_{tr}(a_{t}, s_{t}) \Rightarrow s_{t'}&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;rewards&lt;/em&gt; that maps action and state with its immediate reward &lt;img src=&quot;https://render.githubusercontent.com/render/math?math=f_{rew}(a_{t}, s_{t'}) \Rightarrow r_{t}&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;policy&lt;/em&gt; that maps a state and the next action &lt;img src=&quot;https://render.githubusercontent.com/render/math?math=f_{p}(s_{t}) \Rightarrow a_{t'}&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;autoregression&lt;/em&gt; that looks for correlation between action 1 and action 2 &lt;img src=&quot;https://render.githubusercontent.com/render/math?math=f_{ar}(a_{1, t}) \Rightarrow a_{2, t}&quot; /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We then compute the jacobian of each function in order to differentiate with respect to relevant parameters. For instance, to highlight what part of the state space is directly influenced by agent decisions, we’ll use the jacobian of the transitions function &lt;img src=&quot;https://render.githubusercontent.com/render/math?math=\mathbb{J}({f_{tr}})&quot; /&gt; and differentiate with respect to agent actions.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/ar_policy/jac_f_p1.png&quot; alt=&quot;sources&quot; class=&quot;double-unconst&quot; /&gt;
&lt;img src=&quot;/assets/ar_policy/jac_f_p2.png&quot; alt=&quot;sources&quot; class=&quot;double-unconst&quot; /&gt;&lt;/p&gt;
&lt;center class=&quot;image-foot&quot;&gt;&lt;i&gt;fig. 8: what part of the state space influences decisions the most, for each action a1 (left) and a2 (right). Actions a1 and a2 seem influenced by different parts of the state&lt;/i&gt;&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/ar_policy/jac_f_tr.png&quot; alt=&quot;sources&quot; class=&quot;center&quot; /&gt;&lt;/p&gt;
&lt;center class=&quot;image-foot&quot;&gt;&lt;i&gt;fig. 9: what part of the state space is &quot;controllable&quot; by the policy, for each action (a1, a2) of the space. Here indoor conditions are the most influenced, from both actions&lt;/i&gt;&lt;/center&gt;

&lt;p&gt;Influence of a1 over a2 can also be estimated thanks to computing the jacobian of the forward model that approximates the predictor function that gives a2 knowing a1. This allows to highlight the much stronger influence score of a1 over a2, as shown in &lt;a href=&quot;#fig11&quot;&gt;figure 11&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/ar_policy/jac_f_ar_bar.png&quot; alt=&quot;sources&quot; class=&quot;center&quot; /&gt;&lt;/p&gt;
&lt;center class=&quot;image-foot&quot;&gt;&lt;i&gt;fig. 10: a1 influence on a2, as a scalar score. Left bar for a classic model, right for the AR one&lt;/i&gt;&lt;/center&gt;

&lt;p&gt;We can also estimate how much each action influence the immediate reward. Interestingly, forward models learned on AR and classic policies lead different estimate: model learned on classic policy shows a somewhat equal influence of each action, while model learned on AR policy shows a much stronger influence of a1.&lt;/p&gt;

&lt;p&gt;&lt;a id=&quot;fig11&quot;&gt;&lt;/a&gt;&lt;img src=&quot;/assets/ar_policy/jac_f_rew.png&quot; alt=&quot;sources&quot; class=&quot;center&quot; /&gt;&lt;/p&gt;
&lt;center class=&quot;image-foot&quot;&gt;&lt;i&gt;fig. 11: a1 and a2 influence score on immediate reward, for each policy type&lt;/i&gt;&lt;/center&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;Through a series of examples we’ve demonstrated that autoregressive (AR) policies can bring significant performance and training speed improvements compared to more classic models.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;p&gt;&lt;a id=&quot;1&quot;&gt;[1]&lt;/a&gt;
Mathieu Germain, Karol Gregor, Iain Murray, Hugo Larochelle.
&lt;i&gt;MADE: Masked Autoencoder for Distribution Estimation&lt;/i&gt; &lt;a href=&quot;https://arxiv.org/abs/1502.03509&quot;&gt;2015&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a id=&quot;2&quot;&gt;[2]&lt;/a&gt;
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov.
&lt;i&gt;Proximal Policy Optimization Algorithms&lt;/i&gt; &lt;a href=&quot;https://arxiv.org/abs/1707.06347&quot;&gt;2017&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Wed, 23 Feb 2022 01:00:00 +0100</pubDate>
        <link>http://localhost:4000/hvac/rl/autoregressive/ar_policy.html</link>
        <guid isPermaLink="true">http://localhost:4000/hvac/rl/autoregressive/ar_policy.html</guid>
        
        
        <category>hvac</category>
        
        <category>rl</category>
        
        <category>autoregressive</category>
        
      </item>
    
      <item>
        <title>Nine months of AI-based control optimization on a modern office building HVAC</title>
        <description>&lt;style type=&quot;text/css&quot;&gt;
.center {
    display:block;
    margin: 0 auto;
}
.double-left {
    width:49%;
    display:block;
    float:left;
    margin: 0 auto;
    margin-right: 10px;
}
.add-margin-right {
    margin-right: 20px;
}
.double {
    width:49%;
    margin: 0 auto;
}
.image-foot {
    font-size:10pt;
}
ul {
  display: table;
}
&lt;/style&gt;

&lt;h2 id=&quot;foobot-smart-air-building-sab-project&quot;&gt;Foobot Smart Air Building (SAB) project&lt;/h2&gt;

&lt;p&gt;In October last year, SAB artificial intelligent agents took control over a 15000 m² commercial building HVAC system. This building, located in Northern Europe, was a real challenge:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;a recent (2015), awarded construction. It obtained the &lt;i&gt;DGNB Platinum&lt;/i&gt; construction certification.&lt;/li&gt;
  &lt;li&gt;a modern building management system (BMS), with fancy, state-of-the-art control sequences&lt;/li&gt;
  &lt;li&gt;a low energy consumption baseload for the HVAC system: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;32 kWh/m²·year&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Within a few weeks, we rolled out the SAB HVAC optimization workflow:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;building characteristics and energy &lt;b&gt;data collection&lt;/b&gt;, along with facility managers interview to get a better understanding of known pain points&lt;/li&gt;
  &lt;li&gt;&lt;b&gt;digital twin acquisition&lt;/b&gt;, by crafting a building model calibrated against actual building energy data&lt;/li&gt;
  &lt;li&gt;&lt;b&gt;smart agents training&lt;/b&gt;, by using the digital twin as safe, accurate training environment&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This method allowed us, within a short time frame, to:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;present several optimization strategies, backed by solid energy savings estimates&lt;/li&gt;
  &lt;li&gt;discuss and validate the best strategies&lt;/li&gt;
  &lt;li&gt;design and apply a tailor-made test plan&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;After test plan was fully executed and initial results in line with expectations, our customer gave us the green light for 24/7 control.&lt;/p&gt;

&lt;h2 id=&quot;day-1---a-good-slash&quot;&gt;Day 1 - a good slash&lt;/h2&gt;

&lt;p&gt;Our smart agents’ performance is monitored using opensource tools like Grafana, and aggregated performance Key Performance Indicators (KPI) are continuously computed to track savings, comfort and AQI over the course of several months and years.&lt;/p&gt;

&lt;p&gt;In early fall, we gave our agents the go-live and watched🍿&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/sab_after_9_months/day_1.png&quot; alt=&quot;sources&quot; class=&quot;center&quot; /&gt;&lt;/p&gt;
&lt;center&gt;&lt;i class=&quot;image-foot&quot;&gt;fig. 1: SAB AI agents performance comparison over a 4 days period (source: Grafana dashboard)&lt;/i&gt;&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;This day of October, there was a drastic cut in HVAC energy consumption. All HVAC energy uses (AHU electricity and per-floor heating, showed with different colors in stacked bars chart) were significantly reduced despite similar weather conditions compared to previous, non-SAB days.&lt;/p&gt;

&lt;p&gt;What it demonstrated:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;HVAC was running at design conditions all the time. Air flow rate was too high for building heating and air renewal needs&lt;/li&gt;
  &lt;li&gt;there was too much reheat at floor level. Supplied air temperature was too cold, leading to waste of heating at each floor&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;after-9-months&quot;&gt;After 9 months&lt;/h2&gt;

&lt;p&gt;The first few days went good. But this isn’t enough to demonstrate efficiency of a solution. You need, … time.&lt;/p&gt;

&lt;p&gt;Today, we release some key metrics that show how SAB agents did their job. Starting with the classic Energy v.s. Heating Degree Day scatter and linear regression.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/sab_after_9_months/e_hdd_fit.png&quot; alt=&quot;sources&quot; class=&quot;center&quot; /&gt;&lt;/p&gt;
&lt;center&gt;&lt;i class=&quot;image-foot&quot;&gt;fig. 2: Energy / Heating Degree Day fit&lt;/i&gt;&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;From this first chart, we see SAB is overperforming in every situation, mild or cold weather. The regression slope is steeper for the BMS as HDD increases; it can be interpreted as the HVAC sensitivity to weather, and SAB agents are much less sensitive than the BMS.&lt;/p&gt;

&lt;p&gt;To get a better understanding on each method performance, we can use a piecewise regression (a.k.a segmented regression) to identify some patterns&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/sab_after_9_months/e_w_fit.png&quot; alt=&quot;sources&quot; class=&quot;center&quot; /&gt;&lt;/p&gt;
&lt;center&gt;&lt;i class=&quot;image-foot&quot;&gt;fig. 3: Piecewise energy / weather fit&lt;/i&gt;&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;This time, x axis uses daily mean outdoor temperature. What we observe:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Energy Use Intensity is confirmed smaller&lt;/li&gt;
  &lt;li&gt;heating change point is close to same between BMS and SAB control, at about 10°C. This means that heating needs to kick in only when outdoor temperature is 10°C or below.&lt;/li&gt;
  &lt;li&gt;baseload (horizontal part of the regression) is also improved with SAB: HVAC system, under optimal weather conditions, runs with less&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;takeaways&quot;&gt;Takeaways&lt;/h2&gt;

&lt;p&gt;The figure: &lt;b&gt;a 54% cut in HVAC spending, with no compromise on thermal comfort or indoor air quality&lt;/b&gt;.&lt;/p&gt;

&lt;p&gt;What we learned:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;every building is different, and every tenant has his/her own settings and preferences. You can advise to change set points and tell people to wear a sweater, or you can use a smart system. In present case, thermostat set points weren’t changed and are respected 98% of the time. Air quality is near perfect, with measured CO2 below 900ppm 100% of the time.&lt;/li&gt;
  &lt;li&gt;a successful project requires solid foundations: by giving precise estimates and several optimisation choices to our customers, expectations are set. In present case, actual savings show a 5-10% error with estimates (for winter months, savings were estimated at 50%)&lt;/li&gt;
  &lt;li&gt;visibility is key: we built an alerting and monitoring system that we and our customers use every day. It goes from standard HVAC health checks (like prescribed by ASHRAE guideline 36 for VAV systems) to tailor-made alerts, with auto-remediation, and real-time monitoring. All building sensors data, as well as KPIs, are visible to everyone involved in the project&lt;/li&gt;
  &lt;li&gt;reactivity and security are not optional: if something goes wrong, you need to know it immediately and react as fast as possible. This leads to automating everything. As a consequence, all our control loops deployment pipelines are fully automated, isolated and secured.&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Mon, 21 Jun 2021 02:00:00 +0200</pubDate>
        <link>http://localhost:4000/hvac/control/ai/reinforcement_learning/sab_after_9.html</link>
        <guid isPermaLink="true">http://localhost:4000/hvac/control/ai/reinforcement_learning/sab_after_9.html</guid>
        
        
        <category>hvac</category>
        
        <category>control</category>
        
        <category>ai</category>
        
        <category>reinforcement_learning</category>
        
      </item>
    
      <item>
        <title>HVAC processes control: can AI help?</title>
        <description>&lt;style type=&quot;text/css&quot;&gt;
.center {
    display:block;
    margin: 0 auto;
}
.double-left {
    width:49%;
    display:block;
    float:left;
    margin: 0 auto;
    margin-right: 10px;
}
.add-margin-right {
    margin-right: 20px;
}
.double {
    width:49%;
    margin: 0 auto;
}
.image-foot {
    font-size:10pt;
}
ul {
  display: table;
}
&lt;/style&gt;

&lt;p&gt;Through a series of examples, this article deals with limitations of classic controllers and control strategies in 
&lt;em&gt;Heating, Ventilation and Air Conditioning&lt;/em&gt; (HVAC) systems, and draws a comparison with &lt;em&gt;intelligent agents&lt;/em&gt; (supported
by artificial intelligence techniques) and their benefits. As such, we take as a basis of comparison a common HVAC
system type (&lt;em&gt;Variable Air Volume&lt;/em&gt; or VAV) and show where classic control technics fall short while intelligent agents
bring in better performance.&lt;/p&gt;

&lt;h2 id=&quot;hvac-components-control-optimization-context&quot;&gt;HVAC components control optimization: context&lt;/h2&gt;

&lt;p&gt;According to &lt;a href=&quot;https://www.eia.gov/consumption/commercial/reports/2012/energyusage/&quot;&gt;US Energy Information Agency&lt;/a&gt;, HVAC
systems were responsible for not less than 40% of energy consumption in commercial buildings in 2012. With such a 
significant footprint, everyone has an interest in optimizing their control and operations.&lt;/p&gt;

&lt;p&gt;Below diagram gives a simplified description of such HVAC system, of &lt;em&gt;VAV&lt;/em&gt; type, as we find in many buildings in US,
Europe and Asia since 1980 onwards. This type of system, as its name suggests, varies the amount of air to meet heating,
cooling and air renewal demands in building spaces. While its installation cost is higher than previous generation 
(namely, &lt;em&gt;Constant Air Volume&lt;/em&gt;), it has replaced it advantageously since it brings better comfort guarantees and less
energy expenses. We encounter this in medium to large size buildings mainly, in every kind of facilities.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/pid_vs_ai/ncc_hvac_schematics.svg&quot; alt=&quot;sources&quot; class=&quot;center&quot; /&gt;&lt;/p&gt;
&lt;center&gt;&lt;i class=&quot;image-foot&quot;&gt;fig. 1: simplified schematics of a typical Variable Air Volume (VAV) HVAC system, with central mechanical cooling and hot water reheat&lt;/i&gt;&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;As we can observe, a VAV is a centralized system: most of the ventilation, cooling and heating happens in one place, and
this requires to be configured and operated to meet all demands. By intuition, one can already imagine the challenges
that such systems face: a single (or a handful of) equipment of any kind must satisfy potentially dozens of open spaces,
cell offices, meeting rooms, … This requires proper sizing, configuration and control.&lt;/p&gt;

&lt;p&gt;To do so, HVAC industry relies on processes, hardware and techniques that, for some of them, are decades-old. Given
investment costs to design and promote new equipment or control techniques, but also to replace those already installed,
there is some inertia, legitimate to some extent. But as we’ll discuss in next sections, optimisation or removal of
existing limitations don’t require many capital expenditure anymore. Modern, software-based approaches can
bring dramatic improvements without a full-fledged retrofit.&lt;/p&gt;

&lt;h2 id=&quot;shortcomings-of-classic-controllers-in-hvac-systems&quot;&gt;Shortcomings of classic controllers in HVAC systems&lt;/h2&gt;

&lt;p&gt;To understand what can go wrong, let’s begin our short study by describing classic control systems and strategies in
HVAC systems, backed by concrete examples of sub-optimal control in an actual building.&lt;/p&gt;

&lt;h3 id=&quot;a-classic-controller-case-study-proportional-integral-derivative&quot;&gt;A classic controller case study: Proportional-Integral-Derivative&lt;/h3&gt;

&lt;p&gt;Proportional-Integral-Derivative, or &lt;em&gt;PID&lt;/em&gt;, are the three calculation terms employed in this type of controller to
obtain a process control loop. For many decades, it’s the go-to in industrial processes, and HVAC systems are no
exception.&lt;/p&gt;

&lt;p&gt;However, they have known limitations, that are common to all PID controllers or more specific to HVAC-related processes.
Below we summarize some of them, in order to build a first intuition on what could be optimized.&lt;/p&gt;

&lt;h4 id=&quot;typical-limitations-of-a-pid&quot;&gt;Typical limitations of a PID&lt;/h4&gt;

&lt;p&gt;Let’s take an example with an actual PID controller used to vary supply air fan speed in a VAV system. A common control
strategy is to make the most-open VAV box damper position reach 90%. So this PID has the maximum of all dampers position
as input, a fixed set point of 90%, and outputs fan speed.&lt;/p&gt;

&lt;p&gt;The first limitation is intrinsically coming from how PID works: it requires a margin in the set point to produce stable
results when the controlled process is mechanical (e.g. fan speed can’t go higher than 100% or lower than 0%). When the 
limit of process range is reached, errors accumulate but the controlled process can’t go further in the same direction. 
This is known as &lt;a href=&quot;https://en.wikipedia.org/wiki/Integral_windup&quot;&gt;integral windup&lt;/a&gt;. When it comes to fan control, a 
typical strategy is then to fix the set point below range limit, usually 90%.&lt;/p&gt;

&lt;p&gt;The second limitation is also inherent to PID internals: a large change in set point or in input value will produce
excessive change in the output (mostly due to derivative part of the PID: on a large change, derivative term will be
very large). A common solution is to use ramping either on set point or on PID input (in some “enhanced” controllers).
This produces very slow responses, on a fan start-up that means a long delay before reaching the set point. This can be
observed on below chart, where it takes more than 30min for the PID to reach the set point.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/pid_vs_ai/pid_fan_rampup.png&quot; alt=&quot;sources&quot; class=&quot;center&quot; /&gt;&lt;/p&gt;
&lt;center&gt;&lt;i class=&quot;image-foot&quot;&gt;fig. 2: supply fan speed controlled by a PID and most-open VAV damper position (used as PID input) show a slow 
ramp-up of about 30min&lt;/i&gt;&lt;/center&gt;

&lt;p&gt;Note: ramping is still necessary to deal with mechanical constraints, a common rule is to avoid changes larger than 10%
per minute on fan speed.&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;A third limitation comes from &lt;a href=&quot;https://en.wikipedia.org/wiki/PID_controller#Loop_tuning&quot;&gt;PID gains tuning&lt;/a&gt; which is a
tedious and time-consuming process. Even when controllers come with pre-defined and safe tunings for a particular class
of equipment, the HVAC engineer will have to reconfigure them to reach good performance, since each ventilation system
is unique. However, this tuning, performed once for all (HVAC configurations are only partially revised from time to
time), has about zero chance to cover all operating conditions. This results in typical PID problems like &lt;em&gt;overshooting&lt;/em&gt;
,
&lt;em&gt;oscillations&lt;/em&gt; or &lt;em&gt;hunting&lt;/em&gt;. This can be observed in chart below, where the desired set point is still fixed at 90%&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/pid_vs_ai/pid_fan_oscillations.png&quot; alt=&quot;sources&quot; class=&quot;center&quot; /&gt;&lt;/p&gt;
&lt;center&gt;&lt;i class=&quot;image-foot&quot;&gt;fig. 3: supply fan speed controlled by a PID subject to large oscillations&lt;/i&gt;&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Last but not least, PID controllers are often driven by a single input: in our case, most-open VAV damper position.
However, is this really the goal of a supply fan or did we just find a proxy to accommodate with controller
specifications? The actual real goal is to maintain good thermal and air quality comfort in occupied spaces, while
minimizing energy consumption.&lt;/p&gt;

&lt;p&gt;While the proxy target of 90% can satisfy first part, it has no consideration for energy
savings. How does this translate? This is very usual to find one damper open at 90% while others are way below. This isn’t
just a PID problem (or more specifically, just with the PID controlling the fan), but it’s natural to have zones with
less air demand than others, and zones that are almost always in demand (also known as rogue zones). With a PID, one
can’t do anything about that. Figure 4 illustrates this problem, where a large difference occurs between damper
positions.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/pid_vs_ai/pid_dampers_position.png&quot; alt=&quot;sources&quot; class=&quot;center&quot; /&gt;&lt;/p&gt;
&lt;center&gt;&lt;i class=&quot;image-foot&quot;&gt;fig. 4: supply fan control strategy leading to large differences between VAV boxes dampers position&lt;/i&gt;&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h4 id=&quot;if-only-i-had-known-that-before&quot;&gt;“If only I had known that before…”&lt;/h4&gt;

&lt;p&gt;A PID is only reacting: for a given change in input, it will vary its output to keep as close as possible to its set
point. It has no way to anticipate changes or take proactive actions, since it has neither explicit nor inferred knowledge of
the controlled system. Moreover, its single input gives it a very partial view of the process at hand.&lt;/p&gt;

&lt;p&gt;With such &lt;em&gt;observer model&lt;/em&gt;, anticipation isn’t possible, while simple intuitions could be used to save bits of energy
here and there. A simple example: when HVAC scheduled end of operations is in sight, one could be tempted to slowly
reduce heating or cooling equipment use but not too much to reach end time between desired spaces set points. This is
something a PID can’t do.&lt;/p&gt;

&lt;p&gt;If we generalise this example, &lt;strong&gt;the optimal system should learn building thermal dynamics and discover best control
strategies&lt;/strong&gt;. There are many, and they depend on building use and characteristics.&lt;/p&gt;

&lt;h3 id=&quot;design-conditions-versus-reality&quot;&gt;Design conditions versus reality&lt;/h3&gt;

&lt;p&gt;As stated in &lt;em&gt;Pacific Northwest National Laboratory&lt;/em&gt;
report &lt;a href=&quot;https://www.pnnl.gov/main/publications/external/technical_reports/pnnl-22072.pdf&quot;&gt;Energy Savings for Occupancy-Based Control (OBC) of Variable-Air-Volume (VAV) Systems&lt;/a&gt;:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Each terminal box has a minimum air-flow rate that ensures the ventilation requirements of the occupants of the zone served are met. This minimum air-flow rate is maintained at a constant value based on the design occupancy of the zone, which often corresponds to the maximum occupancy, because measurements of actual occupancy are not currently used to adjust the flow rate. […] In practice, control system integrators and installers often set the cooling minimum air-flow rate for ventilation to between 30% and 50% of the maximum air-flow rate of the terminal box.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The process of &lt;em&gt;Testing, Adjusting and Balancing&lt;/em&gt; (TAB) of a building HVAC system is meant to deliver sufficient amount
of air, and make sure there will be enough heating and cooling capacity. All the calculation and testing is performed
using building &lt;em&gt;design conditions&lt;/em&gt;. In practice, it is undertaken when building is still unoccupied, during one or a
handful of days (so during a particular season, with specific weather conditions). It doesn’t cover the actual use of
the building, which will vary over the course of its life: daily changes - with spaces more or less occupied depending
on the hour of the day - and more profound and permanent changes, like when exceptional conditions arise (Covid-19 and 
its cascade of lockdowns), or simply because tenants’ activity changes.&lt;/p&gt;

&lt;p&gt;Another source of HVAC system misconfiguration and excessive energy use is the small and repetitive adjustments made 
over time by people who are responsible for occupants comfort, or to deal with system instability (operational conditions not
tested during initial setup). While it seems natural they make everything they can to guarantee satisfactory living
conditions, this often results in more energy wasted due to narrowed operational behaviour (e.g. raising minimum set
points).&lt;/p&gt;

&lt;h3 id=&quot;non-linearity-in-controlled-systems&quot;&gt;Non-linearity in controlled systems&lt;/h3&gt;

&lt;p&gt;HVAC systems are (highly) non-linear: they will respond non-linearly depending on controlled equipment, command
magnitude, and current state of the system (for instance: cooling or heating mode).&lt;/p&gt;

&lt;p&gt;A classic example can be drawn from &lt;a href=&quot;https://en.wikipedia.org/wiki/Affinity_laws&quot;&gt;fan affinity laws&lt;/a&gt;: &lt;em&gt;pressure is
proportional to the square of shaft speed&lt;/em&gt;. Basically, as fan speed increases, pressure developed by the fan increases
twice faster. This is only from a theoretical perspective though, and many factors will add to make this pressure/speed
relationship even less linear. What does it mean for a classic controller like a PID? It won’t care, since it has no
knowledge of this, and will vary the output without taking into account that it’s already in the high end or low end of
the range.&lt;/p&gt;

&lt;p&gt;Another example of non-linear system is heat exchanger, that is used in several HVAC components (heat recovery, hot
water heating plates, …). Heat exchange efficiency varies depending on the fluid flow rate between which heat transfer
occurs. How much heat exchange efficiency is non-linear depends on equipment capacity, often rated using a
&lt;em&gt;heat transfer coefficient&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;&lt;i class=&quot;image-foot&quot;&gt;fig. 5: heat transfer efficiency at various flows and T° eff.&lt;/i&gt;
&lt;img src=&quot;/assets/pid_vs_ai/heat_exchange_eff.png&quot; alt=&quot;sources&quot; class=&quot;double-left&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In fig.
5 (&lt;a href=&quot;https://files.danfoss.com/download/Heating/Whitepapers/VFGBC302_optimum-control-of-hex_170407_hires.pdf&quot;&gt;reference&lt;/a&gt;)
, we can see relationship between percentage flow and percentage of heat transferred, for a range of thermal efficiency
between 0.05 and 1 (ideal).&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Notes&lt;/em&gt;:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code&gt;P/P&lt;sub&gt;max&lt;/sub&gt;&lt;/code&gt;: percentage of heat transferred&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;m/m&lt;sub&gt;max&lt;/sub&gt;&lt;/code&gt;: percentage flow&lt;/li&gt;
  &lt;li&gt;thermal efficiency is the ratio between maximum difference in temperature between primary inlet (heating) and
secondary inlet  (heated) fluids, and difference in temperature between primary inlet fluid and primary outlet (
returned to heating system) fluids (equation: &lt;code&gt;&amp;eta;&lt;sub&gt;th&lt;/sub&gt; = (T&lt;sub&gt;11&lt;/sub&gt; - T&lt;sub&gt;12&lt;/sub&gt;) / (T&lt;sub&gt;
11&lt;/sub&gt; - T&lt;sub&gt;21&lt;/sub&gt;)&lt;/code&gt;).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;
Using a linear controller, like a PID to control for instance a heating valve is thus going to provide a sub-optimal performance.&lt;/p&gt;

&lt;p&gt;An HVAC system is composed of many non-linear sub-systems, some interacting together with complex relationships.
&lt;strong&gt;It is therefore very limiting to use a linear controller to control equipment that responds non-linearly and have
interactions on many systems.&lt;/strong&gt;&lt;/p&gt;

&lt;h3 id=&quot;supply-air-temperature-sat-and-the-open-loop-problem&quot;&gt;Supply Air Temperature (SAT) and the open loop problem&lt;/h3&gt;

&lt;p&gt;Another common control point in HVAC systems, that is often a target for control optimisation is Supply Air
Temperature (SAT). SAT at air handling unit level is commonly controlled using a temperature set point. In general, 
the set point must be set, so cooled air arriving at the air terminals is cold enough
to meet cooling demand (but reheated at terminal unit level when necessary). In humid climate, the set point is often
set low so to dehumidify (thanks to condensation) the air and avoid moisture problems.&lt;/p&gt;

&lt;p&gt;What do we need to optimise here? SAT set point will have a significant influence on energy consumed by mechanical
cooling, reheat and also supply fan. In order to minimize energy consumption, this set point can be varied using a 
&lt;em&gt;reset&lt;/em&gt; strategy (commonly referred to as &lt;em&gt;SAT set point reset&lt;/em&gt; strategy).&lt;/p&gt;

&lt;h4 id=&quot;why-is-an-open-loop-a-problem&quot;&gt;Why is an open loop a problem?&lt;/h4&gt;

&lt;p&gt;Although &lt;a href=&quot;https://cbe.berkeley.edu/wp-content/uploads/2019/03/Raftery-CostResponsiveReset-May2017.pdf&quot;&gt;many SAT set point reset strategies exist&lt;/a&gt;
, it is still very common to find outdoor air temperature-based reset strategy (&lt;em&gt;OA reset&lt;/em&gt;) used, even in recent
buildings. The assumption is often made, in cold or mild climates that don’t have high cooling loads, a more
sophisticated approach wouldn’t worth the pain. It’s only partially true because a too cold air will require more
reheat, hence also wasting heating energy. Outdoor air temperature-based reset is an &lt;em&gt;open loop problem&lt;/em&gt;: a linear
relationship is assumed between outdoor air temperature and need for cooling. In practice, a linear equation computes
SAT set point based only on outdoor air temperature, without taking into account the actual indoor conditions, leading
to cold complaints problems and high reheat wastes.&lt;/p&gt;

&lt;h4 id=&quot;any-better-yet&quot;&gt;Any better yet?&lt;/h4&gt;

&lt;p&gt;What about more sophisticated strategies then? They are based on heuristics (like the &lt;em&gt;Trim and Respond&lt;/em&gt; logic recently
advertised in &lt;em&gt;ASHRAE guideline 36&lt;/em&gt;) that vary the set point based on cooling demand: air terminal dampers position
and/or cooling valves position is used as input to compute an increase or a decrease of the SAT set point. While it
brings some indubitable benefits, this has some drawbacks too: such an approach requires thorough tuning and testing (
many variables at stake, like number of ignored requests, thresholds to consider for cooling demand, …) on a
significant period of time (to meet enough variate conditions) and it doesn’t take into account another highly
non-linear aspect: while SAT set point increases to reduce chilled water use, supply fan speed will increase to respond
to unsatisfied cooling needs (if the supply fan speed is variable and also reset, based on static pressure for instance)
. &lt;br /&gt;
Hence, supply fan electricity use will increase, but this method can’t tell when increasing SAT set point stops being
advantageous.&lt;/p&gt;

&lt;p&gt;As we’ve seen, there are many ways HVAC system operations can drift from their intended performance. We scratched the
surface of some challenges HVAC engineers, but also building owners and facility managers, struggle with to keep
occupants satisfied while minimising energy consumption of HVAC systems, which accounts in average for 40% of the total
building energy consumption. We’ll now see why and how &lt;em&gt;Artificial Intelligence&lt;/em&gt; (AI) can bring to help taking back
control.&lt;/p&gt;

&lt;h2 id=&quot;what-does-ai-bring-to-hvac-control&quot;&gt;What does AI bring to HVAC control?&lt;/h2&gt;

&lt;p&gt;For almost a decade now, we find usage of AI in all corners of our digital life: image classification, speech
recognition and synthesis, even fake videos generation. A special class of AI technique has drawn attention starting in
2015, called &lt;em&gt;Deep Reinforcement Learning&lt;/em&gt; (DRL). It differs from “classic” deep learning techniques in that an agent is
trained by interacting with its environment, learning from its mistakes and successes (quantified by a “reward” or a 
“penalty” given to the agent on every action). It has shown incredible successes
with trained agents beating human experts at games like Go or Starcraft, and is used in autonomous vehicles, robotics,
and industry.&lt;/p&gt;

&lt;p&gt;There are some important, fundamental, differences with other learning techniques that make it particularly attractive
for use in process control:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;it can learn in simulated environments, hence data quantity and quality is not a problem. Successful application of 
deep learning has always been limited by lack of good quality data. DRL does not have this limitation.&lt;/li&gt;
  &lt;li&gt;it learns to control: a DRL agent interacts with its environment while trying to maximize the rewards it gets from the
actions it performs. This gives it the ability to test, learn and optimise to their best some control strategies that
no human could ever discover.&lt;/li&gt;
  &lt;li&gt;a domain specialist can encode expert knowledge in the simulated environment and in the reward function to guide the
agent towards expected performance. For instance in a multi-objectives problem, one can choose to reward the agent
more on one objective than another.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;There are many other differences, some of which are specific to current problem at hand in this article. As a matter of
fact, HVAC systems are an excellent target for control optimization using DRL, and this is an active topic of research:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;DeepMind demonstrated this on Google data-centers. There are no official numbers, but Google is now using it generally
to reduce their energy costs. See for instance &lt;a href=&quot;https://deepmind.com/blog/article/deepmind-ai-reduces-google-data-centre-cooling-bill-40&quot;&gt;DeepMind AI Reduces Google Data Centre Cooling Bill by 40%
&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Scientific research on the topic has been very active, as reflects number of papers produced since 2017. A good
example of such a paper
is &lt;a href=&quot;https://ywang393.expressions.syr.edu/wp-content/uploads/2016/07/Deep-reinforcement-learning-for-HVAC-control-in-smart-buildings.pdf&quot;&gt;Deep Reinforcement Learning for Building HVAC Control&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Let’s now detail most important features and compare them with classic HVAC system controllers.&lt;/p&gt;

&lt;h3 id=&quot;seeing-more-than-a-single-input&quot;&gt;Seeing more than a single input&lt;/h3&gt;

&lt;p&gt;A DRL agent can be fed with as many environment observations as wanted (only limited by CPU and memory required to train
and serve the learned control policy). This means it can “see” current indoor temperatures, outdoor conditions,
mechanical and operation status of the system, … Everything that can have an influence of the relevance of its
decisions.&lt;/p&gt;

&lt;p&gt;This makes a significant difference with a classic controller, that usually takes only 1 input. Even if this input is an
aggregation of several observations, this is often insufficient to take an optimal decision. For instance, how to tell
the difference between a damper opened because of a heat demand or a cold demand? This can make a significant difference
in the response to give, depending on a chiller or a heater operational status, and heat exchange properties.&lt;/p&gt;

&lt;h3 id=&quot;learning-system-dynamics&quot;&gt;Learning system dynamics&lt;/h3&gt;

&lt;p&gt;Every building is unique and has its own characteristics in terms of thermal inertia, systems implementation, or
occupancy patterns. More than just learning to cool the air when temperature is high, a DRL agent will learn intrinsic
thermal dynamics of the building, and will be able to give a response in the “correct proportion”, without
overshooting or undershooting.&lt;/p&gt;

&lt;p&gt;A simple example of this is fan speed control with holistic knowledge of the environment state: while PID control
strategy described in previous section only looks at most-open damper, DRL agent “knows” about actual thermal conditions
and all other dampers positions. It can therefore adapt its response appropriately. If there is only 1 zone in demand, it’s 
not the same thing as 2 or 3, as a greater air volume is needed as number of zones increases. A PID would have hard
time not hunting, and as it would take time to stabilise, the system would already be in a different, transient state.&lt;/p&gt;

&lt;p&gt;A benefit of this holistic approach to HVAC control is that one can effectively reduce use of over-sized
equipment, or simply configured to run with very defensive set points. When classic controllers give unsatisfactory
results when system state is at extremes, or simply during transitions, a DRL agent has learned to deal with them
through virtual decades of different, randomised environment conditions.&lt;/p&gt;

&lt;p&gt;Here is a concrete example of these benefits: a HVAC system equipped with a &lt;em&gt;heat recovery machine&lt;/em&gt; saves on heating and
cooling energy by transferring sensible (or total, including latent) heat from return air ducts to supply ones. Heat
transfer efficiency depends on equipment characteristics and air flow rate: basically, the faster the air moves, the
less heat (or cold) can be transferred. A classic controller has no way to take advantage of this property, and will
inevitably push more air as heating or cooling demand increases. A DRL agent controlling the fan will learn that
minimising air flow will improve heat recovery.&lt;/p&gt;

&lt;p&gt;Below graph shows such difference in approach, which leads to a supply air temperature difference of about 5 degrees
between classic and AI control.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/pid_vs_ai/sat_oat_bms_vs_ai.png&quot; alt=&quot;sources&quot; class=&quot;center&quot; /&gt;&lt;/p&gt;
&lt;center&gt;&lt;i class=&quot;image-foot&quot;&gt;fig. 6: difference between supply air temperature when Building Management System (BMS) has 
control (03:00 to 07:00) compared to when AI has control (from 07:00 onwards): a much higher, hence energy efficient 
supply air temperature&lt;/i&gt;&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;anticipation&quot;&gt;Anticipation&lt;/h3&gt;

&lt;p&gt;A DRL agent learns from a reward it gets on every decision it makes. During the training, every action leads to a state
of the system, which is used to compute this reward. The purpose of the training (or teaching) is to make the agent 
&lt;em&gt;maximize the sum of its rewards&lt;/em&gt;. So not every single reward separately, but a couple of them. Intuitively, this
introduces a very fundamental: we are more interested in the overall result than by each decision taken separately, so
the control strategies the agent can discover are very different.&lt;/p&gt;

&lt;p&gt;Some concrete examples:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;agent may take decision that seems bad (in terms of instant reward) but that will lead to much better results on
consecutive ones. For instance, to minimize power demand when energy price is highest, agent may decide to cool spaces
in advance, lowering electricity demand when actual cooling demand should be highest (afternoon).&lt;/li&gt;
  &lt;li&gt;along with learned building thermal dynamics, anticipation of HVAC end of operations (when scheduled) can lead to
interesting strategies: agent will learn to reduce equipment use by just the right proportion until end time is
reached, while maintaining desired comfort. A classic control would just push things as usual, as it has no knowledge
of thermal dynamics or schedules.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Anticipation, as an inherent feature of reinforcement learning, also brings a difference with other AI and deep learning
techniques like forecast. In time series forecast, a model is trained to predict next few minutes or hours of one or
more variables given some observations. This can be used to predict, for instance, what the indoor temperature will be
given past indoor, outdoor and system conditions. With such technique, each decision goes isolated from each other, and
there is no simple way to build strategies that span multiple control steps.&lt;/p&gt;

&lt;h3 id=&quot;testing-hypotheses-and-strategies-in-days-not-months&quot;&gt;Testing hypotheses and strategies in days, not months&lt;/h3&gt;

&lt;p&gt;When a building HVAC system requires a partial or entire retrofit, the project takes from several months to years before
achievement. As it involves hardware changes, and very important expenses, an &lt;em&gt;energy performance audit&lt;/em&gt; is conducted to
assess current building strengths and weaknesses, and prescribe upgrades that would be the most cost effective 
(depending on goals, but &lt;em&gt;return on investment&lt;/em&gt; is often key).&lt;/p&gt;

&lt;p&gt;This is where simulation can take place: using a detailed building model, calibrated against historical energy
consumption, auditors can test various scenarios and simulate their efficiency. It offers a way to make educated, data
driven decisions, and not relying only on experience. The exact same basis can be used to train a deep reinforcement
learning agent to control HVAC equipment: since the building model that serves as training environment is calibrated,
not only the agent is learning from a realistic environment, but its achievements and performance will have solid
ground.&lt;/p&gt;

&lt;p&gt;Figure 7 shows an overview of a &lt;em&gt;digital twin&lt;/em&gt; acquisition process (calibrated building model) used to train deep 
reinforcement learning agents to obtain optimal HVAC control policies, that can be later deployed to control actual 
building HVAC system.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/pid_vs_ai/hvac_training_workflow.png&quot; alt=&quot;sources&quot; class=&quot;center&quot; /&gt;&lt;/p&gt;
&lt;center&gt;&lt;i class=&quot;image-foot&quot;&gt;fig. 7: a digital twin acquisition, DRL agent training and deployment workflow overview&lt;/i&gt;&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;The comparison with classic retrofit projects stops here though: when it will often take months or even years to
finalize an HVAC hardware upgrade, a DRL control policy can be deployed in days. Apart from the obvious improvement in
duration, such work-flow is a real plus for building owners and property managers, since it relies on:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;a clear and standardised modelling and calibration process (e.g. calibration is backed by ASHRAE guideline 14)&lt;/li&gt;
  &lt;li&gt;a cost-effective simulation process to test strategies&lt;/li&gt;
  &lt;li&gt;an additional and valuable asset (the building model itself) that concentrates all necessary information about the
building, for present and future projects&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Simulation engines, as a source of training data for deep reinforcement learning, have other key benefits compared to other solutions:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;they produce an infinite stream of data of excellent quality (historical data can be sparse, noisy, hard to interpret
and parse, sometimes unusable)&lt;/li&gt;
  &lt;li&gt;they give a holistic view of building conditions: you can simulate indoor air quality (rarely available in building
historical data), randomized conditions, etc.&lt;/li&gt;
  &lt;li&gt;agent can be prepared to worst case scenarios: extreme weather, HVAC faults, … Something that is hard to get
otherwise.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Figures 8a and 8b illustrate HVAC optimization proposals one can easily test in a couple of days using a calibrated model
and DRL.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/pid_vs_ai/sim_bench_bar.png&quot; alt=&quot;sources&quot; class=&quot;double-left&quot; /&gt;
&lt;img src=&quot;/assets/pid_vs_ai/sim_bench_tab.png&quot; alt=&quot;sources&quot; class=&quot;double&quot; /&gt;&lt;/p&gt;
&lt;div style=&quot;margin-top:80px; width:100%&quot;&gt;
&lt;center&gt;&lt;i class=&quot;image-foot&quot;&gt;fig. 8a,b: results in percent savings on a benchmark between classic control and AI-based solutions&lt;/i&gt;&lt;/center&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;There’s also a clear difference with forecast-based techniques, which can only rely on historical data from the
building. Historical data from Building Management System (BMS) or other sources is often scarce, if ever present. With
scarce data, you are limited to what the model can learn from, so the number of potential strategies will be small. If
you don’t have enough data, the only thing you can do is… wait! Several months at least, because the model needs to be
trained on different weather conditions to produce satisfactory results. Imagine you collect only data during summer,
what happens in winter when the model is proposed with unseen conditions is likely to be arbitrary decisions. If we
generalize, it’s very likely that a forecast model will take years to be self-sufficient, the time it takes to collect
enough data.&lt;/p&gt;

&lt;h3 id=&quot;additional-benefits&quot;&gt;Additional benefits&lt;/h3&gt;

&lt;p&gt;Something that is harder to quantify, but has reasonable logical ground, is that making control more stable increases
mechanical equipment lifespan. By anticipating needs and taking only one action every few minutes, AI can help avoiding
turning equipment on and off constantly, but most importantly solves poor programming control logics.&lt;/p&gt;

&lt;p&gt;Taking fan speed control as an example, we saw already in previous examples some diseases a fan can suffer from,
like large oscillations throughout the day, likely because of its controller characteristics but also due to how its
input varies: static pressure reset or most-open damper control strategies can bring instability. As a result, the fan
speeds up and slows down throughout the day, wasting energy in acceleration and deceleration, some of which is simply
expelled as heat. This added to the constraint supported by the fan belt, will eventually decrease fan lifespan.&lt;/p&gt;

&lt;p&gt;On the other hand, AI, thanks to its anticipation capability, doesn’t need to constantly react to changes in input. With a
multimillion time steps training, and practical training constraints, a reinforcement learning agent learns to keep
control stable and to avoid large changes. HVAC equipment, with thousands of dollars cost for each piece, is preserved
longer.&lt;/p&gt;

&lt;h3 id=&quot;results-in-an-actual-building&quot;&gt;Results in an actual building&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/pid_vs_ai/hdd_bms_ai_lr.png&quot; alt=&quot;sources&quot; class=&quot;center&quot; /&gt;&lt;/p&gt;
&lt;center&gt;&lt;i class=&quot;image-foot&quot;&gt;fig. 9: performance comparison between control provided by AI and by BMS, 
 based on 18°C-based normalised Heating Degree Days (HDD) over a 90 days period&lt;/i&gt;&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Above chart shows results obtained by multiple deep reinforcement learning agents that took control of HVAC equipment in
an actual commercial building. From day 1, they have been efficient and proved to adapt well to changing conditions.
Taking denormalized results on this 3 months period (100% of the period), they obtained &lt;strong&gt;47% savings in $&lt;/strong&gt; compared to
previous years utilities bills data. Using normalized results (typical office hours), savings were up to &lt;strong&gt;65%&lt;/strong&gt;.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;In this article, we’ve seen how artificial intelligence, and deep reinforcement learning in particular, is well 
suited for HVAC control optimizations. With a cost-effective and robust process based on building models, calibration and 
teaching in simulated environment, efficient control policies can be discovered to achieve significant savings.&lt;/p&gt;
</description>
        <pubDate>Sun, 24 Jan 2021 01:00:00 +0100</pubDate>
        <link>http://localhost:4000/hvac/control/ai/reinforcement_learning/smart_control.html</link>
        <guid isPermaLink="true">http://localhost:4000/hvac/control/ai/reinforcement_learning/smart_control.html</guid>
        
        
        <category>hvac</category>
        
        <category>control</category>
        
        <category>ai</category>
        
        <category>reinforcement_learning</category>
        
      </item>
    
  </channel>
</rss>
