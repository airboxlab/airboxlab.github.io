---
layout: post 
title:  "Autoregressive control policies for HVAC optimization"
date: 2022-02-23 00:00:00 
categories: "hvac rl autoregressive"
comments: false 
author: Antoine Galataud, Alexandre Piché
---

<style type="text/css">
.center {
    display:block;
    margin: 0 auto;
}
.double-left {
    width:49%;
    display:block;
    float:left;
    margin: 0 auto;
    margin-right: 10px;
}
.add-margin-right {
    margin-right: 20px;
}
.double {
    width: 49%;
    margin: 0 auto;
}
.double-unconst {
    width: auto;
    margin: 0 auto;
    margin-left: 1.5rem;
}
.image-foot {
    font-size:10pt;
    max-width: 30rem;
    margin: 0 auto;
}
.small-img {
  width: 45%;
}
ul {
  display: table;
}
</style>

This article deals with the use of autoregressive (AR) models in control policies. After a short background and litterature review, we formulate the problem statement and theoritical advantages over other control strategies involving taking multiple decisions at the same time.

We then demonstrate it can overperform policies with "classic" models on a series of toy tasks (2D grid path-finding) and simulated Heating, Ventilation and Air Conditioning (HVAC) system environments.

## Autoregressive models: a brief introduction

Autoregressive (AR) models are commonly used in statistics and time series problems. They can be defined as regressions where outputs depend on previous values on the same time series. This indicates a strong *temporal* dependency between past and future observations.
Famously known algorithms exist that build on this principle like `ARMA` (Autoregressive Moving Average) and `ARIMA` (Autoregressive Integrated Moving Average).

In the present article we'll give a slightly different meaning to *autoregressive*, although the *temporal* aspect is still there.

### Multi-decisions AR models

In multi-decisions control problems, we often find the following methodology for solving optimization problems that involve applying multiple actions on an environment:
- a single control policy is learnt that models multiple independent action probability distributions
- multiple control policies model one action probability distribution each

In real-world problems, there is often a dependency between multiple decisions taken in the same environment. Here are a few illustrations for control problems involving 2 decisions:
- a bipedal needs to coordinate his left and right legs
- a car driver must coordinate actions on break and accelerator
- ...

In this type of problem, the dependency between actions is strong. That's what AR models can help solving, by modeling a statistical dependency between an action and potentially many others.

This differs from the broader definition of autoregressive models where the statistical dependency is between actions taken at different time steps: action is applied on the environment, result is observed, then a new action is decided that takes into account the previous action and its result on the environment.

Below diagram illustrates these different ways of modeling decision processes with and without AR models

![sources]({{ site.baseurl }}/assets/ar_policy/ar_def.svg){: .center }
<center class="image-foot"><i>fig. 1: 3 ways of modeling decision processes with state / action transitions using independent actions distributions (a.) and AR models (b. and c.)</i></center>
<br/>

Note: nothing prevents from modeling both intra and inter-step dependencies.

### Optimizing multi-decisions processes with Reinforcement Learning

We've briefly described different ways of modeling decision processes that aim at optimizing parameters of multiple actions probability distributions. Reinforcement learning (RL) is, in the family of machine learning / artificial intelligence techniques, a method that can be used to solve this. It is based on a trial-and-error learning process, where an agent tries to learn the optimal policy by interacting with its environment (actions), learning from observations (state) and rewards (also referred as penalties).

Modeling methods described in previous section apply to RL as well, while independent action distributions are the modeling technique that is the most often used.

#### Independently-sampled decisions

This is the most common case. In many classic problems, each action of a multi-decision problem is modeled and its probability distribution is estimated as independent from other actions. This doesn't allow to account for relationships between actions that can be encountered in many problems.

#### Multi-agent

In a multi-agent setting, two or more agents are learning a policy by acting on the same environment. While there are problems with no collaboration needed between agents (each agent decisions has neutral impact on others), in this article we focus on relationships between decisions that have impact on the same shared state.
With this assumption, problem forumlation with multi-agent setup can lead to the well-known "credit assignment dilemma": each agent needs to be rewarded for its action, but how can a fair credit be given to each since they influence the same system?

Below diagram illustrates such a dilemma

![sources]({{ site.baseurl }}/assets/ar_policy/ar_vs_ma.svg){: .center }
<center class="image-foot"><i>fig. 2: 2 agents being rewarded on the same state, leading to a credit assignment problem</i></center>
<br/>

Tricks and heuristics can be found to help with this issue but they can give suboptimal performance, when they exist at all.

## In the litterature

An extensive review of all AR modeling techniques is out of the scope of this article. We focus on AR models in RL problems that aim at modeling dependencies between actions within the same control time step. 

In Harmer et al. (2018)[[1]](#1), imitation learning is used in a 3D game setting that involves multiple actions. However, an assumption is made that actions are conditionnally independent given the state. This simplification reduces computational complexity, but doesn't allow learning relationships between actions.

In Wang et al. (2016)[[2]](#2), an algorithm is proposed to introduce regularization terms in two classic RL methods. The regularization consists of 2 parameters that must be estimated alternatively on every learning step. This proves to improve learning performance on some classic control problems like 2D-grid navigation or Persistent Search and Track (PST).

In Vinyals et al. (2017)[[3]](#3), an extensive problem definition is given about training an agent on StarCraft II 3D-game. In particular, the action space representation is modeled to introduce conditional dependencies between action. (see section 4.2).

Finally, in Germain et al. (2018)[[4]](#4), a masked autoencoder neural network architecture is introduced to estimate parameters of probability distributions. The method uses masking so to respect autoregression constraint, which is to reconstruct an input only from previous inputs. This allows fast and scalable distributions estimation, with any type of ordering or relationship modeling constraints. The study doesn't give any RL application example but it is theoritically usable for the policy network architecture.

## HVAC control sequences applications

HVAC systems are complex and made of multiple systems that interact in complex, non-linear ways (see a brief introduction on that in [HVAC processes control: can AI help?](https://airboxlab.github.io/hvac/control/ai/reinforcement_learning/2021/01/24/smart_control.html)).

HVAC air loops are responsible for maintaining fresh air and good temperatures in buildings occupied zones. They can be seen as a sequence of decisions, starting from how much fresh air the systems lets in, going through deciding at what flow rate it must be pushed and at what temperature, and ending with how this air must be "adjusted" for the target zone.
Many equipment are involved, like fans, heating and cooling coils, chilled and hot water plant loops, etc.

### Why is it a good fit?

Let's build our intuition with an example: a heater is equipped with a hot water coil and a fan, and heats the space by controlling the hot water valve and the fan to blow the hot air. If you turn the fan off but keep valve open, hot air won't move much, and energy consumption will be near 0. If you turn it to maximum position or speed, hot air will move quicker, room will heat quicker too, and energy consumption will be high, both because of the higher fan electricity consumption, and because it increases heat exchange between air and hot water coil.

As a consequence, optimizing control on this simple heating system would require to consider both fan speed and hot water valve position, with prior knowledge that fan and valve have a non-linear dependency regarding temperature of the air blown and energy consumption.

If we generalize this example, many parts of the HVAC system can be seen as dependent to or having a direct or indirect impact on other parts. Trying to control only one equipment as a whole will necessarily lead to suboptimal performance. However, this is often how HVAC systems are controlled: each equipment takes decision regarding a single input. No matter how complex the calculation of this input is, it won't be differentiable since it's a single scalar value. Common calculation methods for control decisions rely on linear methods (linear interpolations, PIDs, ...) with a narrow view of the state of the system.

That's where AR methods and Deep RL can be a good fit: combining function approximation capabilities of neural networks, theoritically unlimited observability of the system and modeling of actions dependencies, the HVAC system and the building it serves could be controlled as a whole (holistic approach), and learned policies could model complex relationships between states and equipment.

### Formulation

Given a policy <img src="https://render.githubusercontent.com/render/math?math=\pi_{\theta}"> to optimize with regards to parameters <img src="https://render.githubusercontent.com/render/math?math=\theta">, state <img src="https://render.githubusercontent.com/render/math?math=s \in \mathcal{S}">, a set of n actions <img src="https://render.githubusercontent.com/render/math?math=a_{i} \in \mathcal{A}">, we can represent the policy as following:
<center><img src="https://render.githubusercontent.com/render/math?math=\pi_{\theta}(a, s) = \prod_{i=1}^{n} \pi_{\theta}(a_{i} \bigm\lvert a_{k<i}, s)"></center><br/>

This simple formulation is generic and describes simple relationships and order with action at rank i being dependent on all previous actions.

An example neural network architecture illustrating this formulation is given below

![sources]({{ site.baseurl }}/assets/ar_policy/ar_nn_arch.svg){: .center }
<center class="image-foot"><i>fig. 3: example neural network architecture to model conditional dependencies between actions</i></center>
<br/>

In this model, a policy with 2 actions is used. It should be noted that:

- `Action 1` and `Action 2` are actually parameters of their respective probability distributions (PD).
- `Action 1` scalar value (sampled from estimated PD) is passed as input for `Action 2` distribution estimation
- `Action 1` *logits* could be passed as input instead of scalar value. This can help preserve information on large discrete/categorical distributions.
- `Action 2` inputs could be solely made of `Action 1` output (scalar or logits). In such case policy can be formulated as <img src="https://render.githubusercontent.com/render/math?math=\pi_{\theta}(a, s) = \pi_{\theta}(a_1, s) \cdot \prod_{i=2}^{n} \pi_{\theta}(a_{i} \bigm\lvert a_{k<i})">

Note that this model doesn't use the masked autoencoder architecture, hence wouldn't scale well for the estimation of a large number of actions distributions.

## Experiments

To validate benefits of such architecture, we compare performance of "classic" network models with the one described in previous section. To draw a fair comparison, the same total number of neural network cells is used in classic and AR policies networks. Furthermore, all other parameters of the Proximal Policy Optimization (PPO, Shulman et al. [[5]](#5)) algorithm are kept equal between the 2 models.

### A path-finding task

The first task is a toy problem where an agent must navigate through a 2D-grid and maximize its reward by moving to certain locations. The action space is designed with 2 actions (vertical and horizontal movements). 
To emphasize on the need to learn correlation between actions, the reward scheme is modeled using 2 gaussian distributions located on 2 corners of the grid, and a one-time bonus whenever the agent reaches a point close to the mean of each gaussian. The agent start each episode starting at the center of the grid. Episode ends when all bonuses are found. We refer to this training environment as bi-modal environment (BME).

![sources]({{ site.baseurl }}/assets/ar_policy/bi_modal_env_grid.png){: .center }
<center class="image-foot"><i>fig. 4: rewards distribution over the BME grid</i></center>
<br/>

We run 5 experiments with different seeds for each method (classic and AR) on the BME and compare the mean return per episode below.

![sources]({{ site.baseurl }}/assets/ar_policy/bi_modal_avg_return.png){: .center }
<center class="image-foot"><i>fig. 5: average episode reward for non AR policy (classic, light green) and AR policy (dark green)</i></center>
<br/>

Visual inspection of trajectories for each method can highlight non-efficiencies and differencies. 
Below animations capture sample trajectories after 50 training iterations.

![sources]({{ site.baseurl }}/assets/ar_policy/bi_modal_classic_traj.gif){: .double }
![sources]({{ site.baseurl }}/assets/ar_policy/bi_modal_ar_traj.gif){: .double }
<center class="image-foot"><i>fig. 6a and 6b: average episode reward for classic policy (left) and AR policy (right)</i></center>
<br/>

On this toy environment, AR policy achieves a much better mean episode reward than its counterpart.

### A simulated HVAC

We now use a simulated building and its HVAC. The building model, or digital twin, is created from an actual building and calibrated on Foobot platform using OpenStudio and EnergyPlus opensource software. It is a 3 storey office building, with a dedicated outdoor air system, a single air handling unit (AHU) with eletric heating and chilled water coil served by a local plant, and fan coil units for zones comfort. 

![sources]({{ site.baseurl }}/assets/ar_policy/dt_3d_geo.png){: .center .small-img }
<center class="image-foot"><i>fig. 7: office building used as a test environment</i></center>
<br/>

A policy is trained with and without autoregressive (AR) actions model. The observation and action spaces, reward function, and PPO parameters are identical between the 2 methods. The action space is composed of 2 actions that command the amount and temperature of air supplied to the zones by the AHU. Each experiment is run for 1e6 timesteps.

![sources]({{ site.baseurl }}/assets/ar_policy/hvac_env_rew.svg){: .double }
![sources]({{ site.baseurl }}/assets/ar_policy/hvac_env_power.svg){: .double }
<br/>
![sources]({{ site.baseurl }}/assets/ar_policy/hvac_env_tmp.svg){: .double }
![sources]({{ site.baseurl }}/assets/ar_policy/hvac_env_co2.svg){: .double }
<center class="image-foot"><i>fig. 8: average episode reward (a., upper left), power demand (b., upper right, in kW), indoor temperature (c., lower left, in Celsius degrees) and CO2 (d., in ppm) for classic and AR policy (blue line) models</i></center>
<br/>

AR policy model not only achieves a better mean episode reward (fig 8a.), but it significantly outperforms (by 8.5%) the energy consumption reduction of the classic method. Other reward components, like thermal comfort and indoor air quality, are also better in line with their respective constraints which are a maximum of 25°C indoor temperature and a maximum of 1000ppm for indoor CO2.

### Analysing impacts and relationships

To understand better our model, a short study is conducted to highlight how action 1 influence action 2, how each action influence the state space, and what part of the state space has the most influence on agent decisions.

To do that, we'll first map our system and learned policy with functions approximations that govern it. They can be seen as forward models that predict the behavior of our policy.

- *transitions* that associates an action and a state to a next state <img src="https://render.githubusercontent.com/render/math?math=f_{tr}(a_{t}, s_{t}) \Rightarrow s_{t'}">
- *rewards* that maps action and state with its immediate reward <img src="https://render.githubusercontent.com/render/math?math=f_{rew}(a_{t}, s_{t'}) \Rightarrow r_{t}">
- *policy* that maps a state and the next action <img src="https://render.githubusercontent.com/render/math?math=f_{p}(s_{t}) \Rightarrow a_{t'}">
- *autoregression* that looks for correlation between action 1 and action 2 <img src="https://render.githubusercontent.com/render/math?math=f_{ar}(a_{1, t}) \Rightarrow a_{2, t}">

We then compute the jacobian of each function in order to differentiate with respect to relevant parameters. For instance, to highlight what part of the state space is directly influenced by agent decisions, we'll use the jacobian of the transitions function <img src="https://render.githubusercontent.com/render/math?math=\mathbb{J}({f_{tr}})"/> and differentiate with respect to agent actions.

![sources]({{ site.baseurl }}/assets/ar_policy/jac_f_p1.png){: .double-unconst }
![sources]({{ site.baseurl }}/assets/ar_policy/jac_f_p2.png){: .double-unconst }
<center class="image-foot"><i>fig. 9: what part of the state space influences decisions the most, for each action a1 (left) and a2 (right). Actions a1 and a2 seem influenced by different parts of the state</i></center><br/>

![sources]({{ site.baseurl }}/assets/ar_policy/jac_f_tr.png){: .center }
<center class="image-foot"><i>fig. 10: what part of the state space is "controllable" by the policy, for each action (a1, a2) of the space. Here indoor conditions are the most influenced, from both actions</i></center>

Influence of a1 over a2 can also be estimated thanks to computing the jacobian of the forward model that approximates the predictor function that gives a2 knowing a1. This allows to highlights the much stronger influence score of a1 over a2, as shown in figure 11.

![sources]({{ site.baseurl }}/assets/ar_policy/jac_f_ar_bar.png){: .center }
<center class="image-foot"><i>fig. 11: a1 influence on a2, as a scalar score. Left bar for a classic model, right for the AR one</i></center>

We can also estimate how much each action influence the immediate reward. Interestingly, forward models learned on AR and classic policies lead different estimate: model learned on classic policy shows a somewhat equal influence of each action, while model learned on AR policy shows a much stronger influence of a1.

![sources]({{ site.baseurl }}/assets/ar_policy/jac_f_rew.png){: .center }
<center class="image-foot"><i>fig. 12: a1 and a2 influence score on immediate reward, for each policy type</i></center>

## Conclusion

Through a series of examples we've demonstrated that autoregressive (AR) policies can bring significant performance and training speed improvements compared to more classic models.

## References

<a id="1">[1]</a> 
Jack Harmer, Linus Gisslén, Jorge del Val, Henrik Holst, Joakim Bergdahl, Tom Olsson, Kristoffer Sjöö, Magnus Nordin. 
<i>Imitation Learning with Concurrent Actions in 3D Games.</i> [2018](https://arxiv.org/abs/1803.05402)

<a id="2">[2]</a>
Han Wang and Yang Yu.
<i>Exploring Multi-Action Relationship in Reinforcement Learning.</i> [2016](http://www.lamda.nju.edu.cn/wanghan/pricai16.pdf)

<a id="3">[3]</a>
Oriol Vinyals, Timo Ewalds, Sergey Bartunov, Petko Georgiev, Alexander Sasha Vezhnevets, Michelle Yeo, Alireza Makhzani, Heinrich Küttler, John Agapiou, Julian Schrittwieser, John Quan, Stephen Gaffney, Stig Petersen, Karen Simonyan, Tom Schaul, Hado van Hasselt, David Silver, Timothy Lillicrap, Kevin Calderone, Paul Keet, Anthony Brunasso, David Lawrence, Anders Ekermo, Jacob Repp, Rodney Tsing.
<i>StarCraft II: A New Challenge for Reinforcement Learning.</i> [2017](https://arxiv.org/abs/1708.04782)

<a id="4">[4]</a>
Mathieu Germain, Karol Gregor, Iain Murray, Hugo Larochelle.
<i>MADE: Masked Autoencoder for Distribution Estimation</i> [2015](https://arxiv.org/abs/1502.03509)

<a id="5">[5]</a>
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov.
<i>Proximal Policy Optimization Algorithms</i> [2017](https://arxiv.org/abs/1707.06347)

